#!/usr/bin/env python3
"""
åŸºäºAIçš„CAPLæµ‹è¯•ç”¨ä¾‹æ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä»åŠŸèƒ½å®Œæ•´æ€§è§’åº¦è¯„ä¼°æµ‹è¯•ç”¨ä¾‹è´¨é‡
"""

import os
import sys
import json
import time
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import ollama
from dotenv import load_dotenv
from dataclasses import dataclass, asdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

@dataclass
class AIEvaluationResult:
    """AIè¯„ä¼°ç»“æœ"""
    functional_completeness: float  # åŠŸèƒ½å®Œæ•´æ€§è¯„åˆ† (0-100)
    requirement_coverage: float     # éœ€æ±‚è¦†ç›–ç‡ (0-100)
    test_logic_correctness: float   # æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§ (0-100)
    edge_case_handling: float       # è¾¹ç•Œæ¡ä»¶å¤„ç† (0-100)
    error_handling: float           # é”™è¯¯å¤„ç†è¯„ä¼° (0-100)
    code_quality: float            # ä»£ç è´¨é‡ (0-100)
    missing_functionalities: List[str]  # ç¼ºå¤±çš„åŠŸèƒ½ç‚¹
    redundant_tests: List[str]          # å†—ä½™çš„æµ‹è¯•
    improvement_suggestions: List[str]   # æ”¹è¿›å»ºè®®
    detailed_analysis: str              # è¯¦ç»†åˆ†æ

class CAPLAIEvaluator:
    """CAPLæµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°å™¨"""
    
    def __init__(self, model_type: str = None, api_url: str = None, model_name: str = None, api_key: str = None):
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–é…ç½®
        self.model_type = model_type or os.getenv('API_TYPE', 'ollama')
        self.api_url = api_url or self._get_default_api_url()
        self.model_name = model_name or self._get_default_model()
        self.api_key = api_key or os.getenv('API_KEY')
        
        # å…¶ä»–é…ç½®å‚æ•°
        self.context_length = int(os.getenv('OLLAMA_CONTEXT_LENGTH', '8192'))
        self.max_tokens = int(os.getenv('OLLAMA_MAX_TOKENS', '4096'))
        self.temperature = float(os.getenv('TEMPERATURE', '0.3'))
        self.top_p = float(os.getenv('TOP_P', '0.5'))
        
        self.system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦ç”µå­æµ‹è¯•ä¸“å®¶ï¼Œä¸“é—¨è¯„ä¼°CAPLæµ‹è¯•ç”¨ä¾‹çš„è´¨é‡ã€‚
        è¯·ä»åŠŸèƒ½å®Œæ•´æ€§è§’åº¦åˆ†ææµ‹è¯•ç”¨ä¾‹ï¼Œé‡ç‚¹å…³æ³¨ï¼š
        1. æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰åŠŸèƒ½éœ€æ±‚
        2. æµ‹è¯•é€»è¾‘æ˜¯å¦æ­£ç¡®
        3. è¾¹ç•Œæ¡ä»¶æ˜¯å¦å……åˆ†è€ƒè™‘
        4. é”™è¯¯å¤„ç†æ˜¯å¦å®Œå–„
        5. æµ‹è¯•ç”¨ä¾‹æ˜¯å¦å†—ä½™
        
        è¯·æä¾›å…·ä½“çš„è¯„åˆ†å’Œæ”¹è¿›å»ºè®®ã€‚
        """
    
    def _get_default_api_url(self) -> str:
        """è·å–é»˜è®¤APIåœ°å€"""
        if self.model_type == 'ollama':
            return os.getenv('API_URL', 'http://localhost:11434')
        else:  # openaiå…¼å®¹æ¥å£
            return os.getenv('API_URL', 'http://localhost:1234/v1')
    
    def _get_default_model(self) -> str:
        """è·å–é»˜è®¤æ¨¡å‹åç§°"""
        if self.model_type == 'ollama':
            return os.getenv('OLLAMA_MODEL', 'qwen3:8b')
        else:  # openaiå…¼å®¹æ¥å£
            return os.getenv('OPENAI_MODEL', 'qwen/qwen3-8b')
    
    def read_file_content(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def extract_requirements_from_md(self, md_content: str) -> List[str]:
        """ä»éœ€æ±‚æ–‡æ¡£æå–åŠŸèƒ½éœ€æ±‚"""
        requirements = []
        
        # æå–æµ‹è¯•æ­¥éª¤ä¸­çš„åŠŸèƒ½æè¿°
        lines = md_content.split('\n')
        for line in lines:
            if '|' in line and 'æµ‹è¯•æ­¥éª¤' not in line and 'é¢„æœŸç»“æœ' not in line:
                parts = [part.strip() for part in line.split('|')]
                if len(parts) >= 4:
                    test_step = parts[2] if len(parts) > 2 else ""
                    expected_result = parts[3] if len(parts) > 3 else ""
                    if test_step and test_step != 'æµ‹è¯•æ­¥éª¤':
                        requirements.append({
                            'step': test_step,
                            'expected': expected_result,
                            'functional_requirement': self._extract_functional_requirement(test_step)
                        })
        
        return requirements
    
    def _extract_functional_requirement(self, test_step: str) -> str:
        """æå–åŠŸèƒ½éœ€æ±‚"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å¯æ›´å¤æ‚
        keywords = [
            'é›¨åˆ·', 'wiper', 'é€Ÿåº¦', 'speed', 'ä½ç½®', 'position',
            'é—´æ­‡', 'intermittent', 'ä½é€Ÿ', 'low speed', 'é«˜é€Ÿ', 'high speed',
            'åœæ­¢', 'stop', 'å¯åŠ¨', 'start', 'æ•…éšœ', 'fault'
        ]
        
        functional_req = []
        for keyword in keywords:
            if keyword.lower() in test_step.lower():
                functional_req.append(keyword)
        
        return " ".join(functional_req) if functional_req else test_step
    
    def create_evaluation_prompt(self, handwritten_content: str, generated_content: str, requirements: List[str]) -> str:
        """åˆ›å»ºAIè¯„ä¼°æç¤º"""
        
        requirements_text = "\n".join([
            f"- {req['step']} -> é¢„æœŸ: {req['expected']}"
            for req in requirements
        ])
        
        prompt = f"""
        è¯·ä½œä¸ºCAPLæµ‹è¯•ä¸“å®¶ï¼Œä»åŠŸèƒ½å®Œæ•´æ€§è§’åº¦è¯„ä¼°ä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹ã€‚
        
        ## éœ€æ±‚æ–‡æ¡£
        {requirements_text}
        
        ## æ‰‹å†™æµ‹è¯•ç”¨ä¾‹
        ```capl
        {handwritten_content}
        ```
        
        ## ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        ```capl
        {generated_content}
        ```
        
        è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼Œæ¯é¡¹ç»™å‡º0-100çš„è¯„åˆ†ï¼š
        
        1. **åŠŸèƒ½å®Œæ•´æ€§** (functional_completeness): æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰åŠŸèƒ½éœ€æ±‚
        2. **éœ€æ±‚è¦†ç›–ç‡** (requirement_coverage): éœ€æ±‚æ–‡æ¡£ä¸­çš„åŠŸèƒ½ç‚¹è¦†ç›–ç¨‹åº¦
        3. **æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§** (test_logic_correctness): æµ‹è¯•é€»è¾‘æ˜¯å¦ç¬¦åˆä¸šåŠ¡è§„åˆ™
        4. **è¾¹ç•Œæ¡ä»¶å¤„ç†** (edge_case_handling): æ˜¯å¦è€ƒè™‘äº†è¾¹ç•Œå€¼å’Œå¼‚å¸¸æƒ…å†µ
        5. **é”™è¯¯å¤„ç†** (error_handling): å¯¹é”™è¯¯æƒ…å†µçš„å¤„ç†æ˜¯å¦å®Œå–„
        6. **ä»£ç è´¨é‡** (code_quality): ä»£ç å¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
        {{
            "functional_completeness": åˆ†æ•°,
            "requirement_coverage": åˆ†æ•°,
            "test_logic_correctness": åˆ†æ•°,
            "edge_case_handling": åˆ†æ•°,
            "error_handling": åˆ†æ•°,
            "code_quality": åˆ†æ•°,
            "missing_functionalities": ["ç¼ºå¤±çš„åŠŸèƒ½ç‚¹åˆ—è¡¨"],
            "redundant_tests": ["å†—ä½™çš„æµ‹è¯•åˆ—è¡¨"],
            "improvement_suggestions": ["æ”¹è¿›å»ºè®®åˆ—è¡¨"],
            "detailed_analysis": "è¯¦ç»†åˆ†ææ–‡æœ¬"
        }}
        """
        
        return prompt
    
    def call_ai_model(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨AIæ¨¡å‹è¿›è¡Œè¯„ä¼°"""
        try:
            if self.model_type == "ollama":
                return self._call_ollama(prompt)
            else:  # openaiå…¼å®¹æ¥å£
                return self._call_openai_compatible(prompt)
        except Exception as e:
            print(f"AIæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return self._get_default_result()
    
    def _call_openai_compatible(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨OpenAIå…¼å®¹æ¥å£"""
        # æ„å»ºå®Œæ•´çš„APIç«¯ç‚¹URL
        if self.api_url.endswith('/v1'):
            api_url = f"{self.api_url}/chat/completions"
        elif not self.api_url.endswith('/chat/completions'):
            api_url = f"{self.api_url.rstrip('/')}/chat/completions"
        else:
            api_url = self.api_url
        
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            "stream": False,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "max_tokens": self.max_tokens
        }
        
        try:
            response = requests.post(api_url, json=payload, headers=headers, timeout=120)
            response.raise_for_status()
            
            data = response.json()
            content = data['choices'][0]['message']['content']
            
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            
            return json.loads(content.strip())
        except requests.exceptions.ConnectionError as e:
            raise ConnectionError(f"è¿æ¥å¤±è´¥ - è¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨ (è¿è¡Œ 'ollama serve' æˆ–å¯åŠ¨ LM Studio): {e}")
        except requests.exceptions.Timeout as e:
            raise TimeoutError(f"è¯·æ±‚è¶…æ—¶ - è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒæœåŠ¡çŠ¶æ€: {e}")
        except json.JSONDecodeError as e:
            raise ValueError(f"AIå“åº”æ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            error_msg = str(e)
            if "Connection" in error_msg or "connect" in error_msg.lower():
                raise ConnectionError(f"è¿æ¥å¤±è´¥ - è¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨å¹¶æ­£åœ¨ç›‘å¬æ­£ç¡®çš„ç«¯å£: {e}")
            elif "404" in error_msg:
                raise ValueError(f"æ¨¡å‹æœªæ‰¾åˆ° - è¯·ç¡®ä¿æ¨¡å‹å·²åŠ è½½: {e}")
            else:
                raise RuntimeError(f"AIè°ƒç”¨å¤±è´¥: {e}")
    
    def _call_ollama(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨Ollamaæœ¬åœ°æ¨¡å‹"""
        try:
            # ä½¿ç”¨å®˜æ–¹ ollama åº“
            ollama_host = self.api_url.rstrip('/')
            client = ollama.Client(host=ollama_host)
            
            response = client.chat(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": prompt}
                ],
                options={
                    "temperature": self.temperature,
                    "top_p": self.top_p,
                    "num_ctx": self.context_length,
                    "num_predict": self.max_tokens
                }
            )
            
            content = response['message']['content']
            
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            
            return json.loads(content.strip())
        except Exception as e:
            error_msg = str(e)
            if "Connection" in error_msg or "connect" in error_msg.lower():
                raise ConnectionError(f"è¿æ¥å¤±è´¥ - è¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨ (è¿è¡Œ 'ollama serve'): {e}")
            elif "404" in error_msg:
                raise ValueError(f"æ¨¡å‹æœªæ‰¾åˆ° - è¯·è¿è¡Œ 'ollama run {self.model_name}' åŠ è½½æ¨¡å‹: {e}")
            else:
                raise RuntimeError(f"Ollamaè°ƒç”¨å¤±è´¥: {e}")
    
    def _get_default_result(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤è¯„ä¼°ç»“æœ"""
        return {
            "functional_completeness": 50.0,
            "requirement_coverage": 50.0,
            "test_logic_correctness": 50.0,
            "edge_case_handling": 50.0,
            "error_handling": 50.0,
            "code_quality": 50.0,
            "missing_functionalities": ["æ— æ³•è¿æ¥AIæ¨¡å‹è¿›è¡Œè¯„ä¼°"],
            "redundant_tests": [],
            "improvement_suggestions": ["è¯·æ£€æŸ¥AIæ¨¡å‹é…ç½®"],
            "detailed_analysis": "ç”±äºAIæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œæ— æ³•æä¾›è¯¦ç»†åˆ†æã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚"
        }
    
    def evaluate_testcase(self, testcase_id: str, handwritten_path: str, generated_path: str, requirement_path: str) -> AIEvaluationResult:
        """è¯„ä¼°å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        handwritten_content = self.read_file_content(handwritten_path)
        generated_content = self.read_file_content(generated_path)
        requirement_content = self.read_file_content(requirement_path)
        
        if not all([handwritten_content, generated_content, requirement_content]):
            print("âŒ éƒ¨åˆ†æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯»å–")
            return AIEvaluationResult(**self._get_default_result())
        
        # æå–éœ€æ±‚
        requirements = self.extract_requirements_from_md(requirement_content)
        
        # åˆ›å»ºè¯„ä¼°æç¤º
        prompt = self.create_evaluation_prompt(handwritten_content, generated_content, requirements)
        
        # è°ƒç”¨AIæ¨¡å‹è¯„ä¼°
        ai_result = self.call_ai_model(prompt)
        
        return AIEvaluationResult(**ai_result)
    
    def generate_detailed_report(self, result: AIEvaluationResult, testcase_id: str) -> str:
        """ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""
        
        # è®¡ç®—åŠ æƒç»¼åˆè¯„åˆ†
        weighted_score = (result.functional_completeness * 0.25 + 
                         result.requirement_coverage * 0.25 + 
                         result.test_logic_correctness * 0.20 + 
                         result.edge_case_handling * 0.15 + 
                         result.error_handling * 0.10 + 
                         result.code_quality * 0.05)
        rating = self._get_rating(weighted_score)
        
        report = f"""\# CAPLæµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°æŠ¥å‘Š - æµ‹è¯•ç”¨ä¾‹ {testcase_id}

## ç»¼åˆè¯„åˆ†
| è¯„ä¼°ç»´åº¦ | å¾—åˆ† | è¯„çº§ |
|----------|------|------|
| åŠŸèƒ½å®Œæ•´æ€§ | {result.functional_completeness:.1f}/100 | {self._get_rating(result.functional_completeness)} |
| éœ€æ±‚è¦†ç›–ç‡ | {result.requirement_coverage:.1f}/100 | {self._get_rating(result.requirement_coverage)} |
| æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§ | {result.test_logic_correctness:.1f}/100 | {self._get_rating(result.test_logic_correctness)} |
| è¾¹ç•Œæ¡ä»¶å¤„ç† | {result.edge_case_handling:.1f}/100 | {self._get_rating(result.edge_case_handling)} |
| é”™è¯¯å¤„ç† | {result.error_handling:.1f}/100 | {self._get_rating(result.error_handling)} |
| ä»£ç è´¨é‡ | {result.code_quality:.1f}/100 | {self._get_rating(result.code_quality)} |

## ç»¼åˆè¯„åˆ†: {weighted_score:.1f}/100 ({rating})

## è¯¦ç»†åˆ†æ
{result.detailed_analysis}

"""
        
        if result.missing_functionalities:
            report += f"""
## ç¼ºå¤±çš„åŠŸèƒ½ç‚¹
{chr(10).join([f"- {func}" for func in result.missing_functionalities])}
"""
        
        if result.redundant_tests:
            report += f"""
## å†—ä½™çš„æµ‹è¯•
{chr(10).join([f"- {test}" for test in result.redundant_tests])}
"""
        
        if result.improvement_suggestions:
            report += f"""
## æ”¹è¿›å»ºè®®
{chr(10).join([f"- {suggestion}" for suggestion in result.improvement_suggestions])}
"""
        
        return report
    
    def _get_rating(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è¿”å›è¯„çº§"""
        if score >= 90:
            return "ä¼˜ç§€"
        elif score >= 80:
            return "è‰¯å¥½"
        elif score >= 70:
            return "ä¸­ç­‰"
        elif score >= 60:
            return "åŠæ ¼"
        else:
            return "éœ€æ”¹è¿›"
    
    def save_evaluation_result(self, result: AIEvaluationResult, testcase_id: str, output_dir: str = "results"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå¹´æœˆæ—¥æ—¶åˆ†ç§’æ ¼å¼çš„æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        
        # ä¿å­˜JSONç»“æœ
        json_file = output_path / f"ai_evaluation_{testcase_id}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = output_path / f"ai_report_{testcase_id}_{timestamp}.md"
        report = self.generate_detailed_report(result, testcase_id)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“Š AIè¯„ä¼°ç»“æœå·²ä¿å­˜:")
        print(f"  JSONæ–‡ä»¶: {json_file}")
        print(f"  æŠ¥å‘Šæ–‡ä»¶: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='CAPLæµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°ç³»ç»Ÿ')
    parser.add_argument('testcase_id', help='æµ‹è¯•ç”¨ä¾‹ID')
    parser.add_argument('--model-type', choices=['ollama', 'openai'], default=None, help='AIæ¨¡å‹ç±»å‹ (ollama æˆ– openaiå…¼å®¹æ¥å£)')
    parser.add_argument('--api-url', help='APIæœåŠ¡åœ°å€')
    parser.add_argument('--model', help='ä½¿ç”¨çš„æ¨¡å‹åç§°')
    parser.add_argument('--context-length', type=int, help='ä¸Šä¸‹æ–‡é•¿åº¦ (tokens)')
    parser.add_argument('--max-tokens', type=int, help='æœ€å¤§è¾“å‡ºtokensæ•°')
    parser.add_argument('--temperature', type=float, help='ç”Ÿæˆæ¸©åº¦ (0.0-1.0)')
    parser.add_argument('--top-p', type=float, help='top-pé‡‡æ ·å‚æ•° (0.0-1.0)')
    
    args = parser.parse_args()
    
    # æŸ¥æ‰¾æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶
    base_dir = Path("/Users/robertsong/Downloads/code/testcase_AI_generator")
    
    handwritten_path = base_dir / "test_output" / f"testcase_id_{args.testcase_id}.can"
    generated_path = base_dir / "test_output" / f"qualification_*{args.testcase_id}*.can"
    requirement_path = base_dir / "pdf_converter" / "testcases" / f"qualification_*{args.testcase_id}*.md"
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶
    generated_files = list(base_dir.glob(f"test_output/qualification*{args.testcase_id}*.can"))
    requirement_files = list(base_dir.glob(f"pdf_converter/testcases/qualification*{args.testcase_id}*.md"))
    
    if not generated_files:
        print(f"âŒ æœªæ‰¾åˆ°ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶")
        return
    
    if not requirement_files:
        print(f"âŒ æœªæ‰¾åˆ°éœ€æ±‚æ–‡æ¡£")
        return
    
    generated_path = generated_files[0]
    requirement_path = requirement_files[0]
    
    if not handwritten_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°æ‰‹å†™æµ‹è¯•ç”¨ä¾‹: {handwritten_path}")
        return
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CAPLAIEvaluator(
        model_type=args.model_type,
        api_url=args.api_url,
        model_name=args.model
    )
    
    # æ‰§è¡Œè¯„ä¼°
    print(f"ğŸ¤– å¼€å§‹AIè¯„ä¼°æµ‹è¯•ç”¨ä¾‹ {args.testcase_id}...")
    result = evaluator.evaluate_testcase(
        args.testcase_id,
        str(handwritten_path),
        str(generated_path),
        str(requirement_path)
    )
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_result(result, args.testcase_id)
    
    # æ‰“å°ç®€è¦ç»“æœ
    print(f"\nğŸ“Š AIè¯„ä¼°å®Œæˆ!")
    print(f"åŠŸèƒ½å®Œæ•´æ€§: {result.functional_completeness:.1f}/100")
    print(f"éœ€æ±‚è¦†ç›–ç‡: {result.requirement_coverage:.1f}/100")
    print(f"æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§: {result.test_logic_correctness:.1f}/100")

if __name__ == "__main__":
    main()