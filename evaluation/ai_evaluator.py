#!/usr/bin/env python3
"""
åŸºäºAIçš„CAPLæµ‹è¯•ç”¨ä¾‹æ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ
ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä»åŠŸèƒ½å®Œæ•´æ€§è§’åº¦è¯„ä¼°æµ‹è¯•ç”¨ä¾‹è´¨é‡
"""

import os
import sys
import json
import time
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import ollama
from dotenv import load_dotenv
from dataclasses import dataclass, asdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

@dataclass
class AIEvaluationResult:
    """AIè¯„ä¼°ç»“æœ"""
    functional_completeness: float  # åŠŸèƒ½å®Œæ•´æ€§è¯„åˆ† (0-100)
    testspec_coverage: float     # æµ‹è¯•è¦†ç›–ç‡ (0-100)
    test_logic_correctness: float   # æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§ (0-100)
    edge_case_handling: float       # è¾¹ç•Œæ¡ä»¶å¤„ç† (0-100)
    error_handling: float           # é”™è¯¯å¤„ç†è¯„ä¼° (0-100)
    code_quality: float            # ä»£ç è´¨é‡ (0-100)
    missing_functionalities: List[str]  # ç¼ºå¤±çš„åŠŸèƒ½ç‚¹
    redundant_tests: List[str]          # å†—ä½™çš„æµ‹è¯•
    improvement_suggestions: List[str]   # æ”¹è¿›å»ºè®®
    detailed_analysis: str              # è¯¦ç»†åˆ†æ
    scoring_basis: Dict[str, str] = None  # è¯„åˆ†ä¾æ®ï¼Œå¯é€‰å­—æ®µ

class CAPLAIEvaluator:
    """CAPLæµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°å™¨"""
    
    def __init__(self, model_type: str = None, api_url: str = None, model_name: str = None, api_key: str = None, verbose: bool = False):
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # ä»ç¯å¢ƒå˜é‡æˆ–å‚æ•°è·å–é…ç½®
        self.model_type = model_type or os.getenv('API_TYPE', 'ollama')
        self.api_url = api_url or self._get_default_api_url()
        self.model_name = model_name or self._get_default_model()
        self.api_key = api_key or os.getenv('API_KEY')
        self.verbose = verbose
        
        # ä¼˜åŒ–å‚æ•°ä»¥æé«˜ä¸€è‡´æ€§
        self.context_length = int(os.getenv('OLLAMA_CONTEXT_LENGTH', '8192'))
        self.max_tokens = int(os.getenv('OLLAMA_MAX_TOKENS', '4096'))
        self.temperature = float(os.getenv('EVALUATOR_TEMPERATURE', '0.05'))  # æä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
        self.top_p = float(os.getenv('EVALUATOR_TOP_P', '0.8'))
        
        # æ‰“å°æ‰€æœ‰å‚æ•°å€¼ç”¨äºè°ƒè¯•
        print("ğŸ“Š AIè¯„ä¼°å™¨é…ç½®å‚æ•°:")
        print(f"   æ¨¡å‹ç±»å‹: {self.model_type}")
        print(f"   APIåœ°å€: {self.api_url}")
        print(f"   æ¨¡å‹åç§°: {self.model_name}")
        print(f"   APIå¯†é’¥: {'å·²é…ç½®' if self.api_key else 'æœªé…ç½®'}")
        print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {self.context_length}")
        print(f"   æœ€å¤§tokenæ•°: {self.max_tokens}")
        print(f"   æ¸©åº¦å‚æ•°: {self.temperature}")
        print(f"   Top-På‚æ•°: {self.top_p}")
        print(f"   è¯¦ç»†æ¨¡å¼: {self.verbose}")
        print("-" * 50)
        
        self.system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ±½è½¦ç”µå­æµ‹è¯•ä¸“å®¶ï¼Œä¸“é—¨è¯„ä¼°CAPLæµ‹è¯•ç”¨ä¾‹çš„è´¨é‡ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†è¿›è¡Œè¯„åˆ†ï¼Œç¡®ä¿æ¯æ¬¡è¯„ä¼°çš„ä¸€è‡´æ€§ã€‚

        ## è¯„åˆ†æ ‡å‡†ï¼ˆä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†è¯„åˆ†ï¼‰

        ### åŠŸèƒ½å®Œæ•´æ€§è¯„åˆ†æ ‡å‡†ï¼š
        - 100åˆ†ï¼šå®Œå…¨è¦†ç›–æ‰€æœ‰åŠŸèƒ½æµ‹è¯•ï¼ŒåŒ…æ‹¬ä¸»è¦åŠŸèƒ½ã€æ¬¡è¦åŠŸèƒ½ã€è¾¹ç¼˜åŠŸèƒ½
        - 90-99åˆ†ï¼šåŸºæœ¬è¦†ç›–æ‰€æœ‰ä¸»è¦åŠŸèƒ½ï¼Œå°‘é‡æ¬¡è¦åŠŸèƒ½ç¼ºå¤±
        - 80-89åˆ†ï¼šè¦†ç›–ä¸»è¦åŠŸèƒ½ï¼Œä½†æœ‰æ˜æ˜¾åŠŸèƒ½ç¼ºå¤±
        - 70-79åˆ†ï¼šéƒ¨åˆ†ä¸»è¦åŠŸèƒ½æœªè¦†ç›–
        - 60-69åˆ†ï¼šå¤§é‡åŠŸèƒ½æµ‹è¯•ç¼ºå¤±
        - <60åˆ†ï¼šåŠŸèƒ½è¦†ç›–ä¸¥é‡ä¸è¶³

        ### æµ‹è¯•è¦†ç›–ç‡è¯„åˆ†æ ‡å‡†ï¼š
        - 100åˆ†ï¼šæµ‹è¯•æ–‡æ¡£ä¸­100%åŠŸèƒ½ç‚¹éƒ½æœ‰å¯¹åº”æµ‹è¯•
        - 95-99åˆ†ï¼šæµ‹è¯•æ–‡æ¡£ä¸­95-99%åŠŸèƒ½ç‚¹æœ‰å¯¹åº”æµ‹è¯•
        - 90-94åˆ†ï¼šæµ‹è¯•æ–‡æ¡£ä¸­90-94%åŠŸèƒ½ç‚¹æœ‰å¯¹åº”æµ‹è¯•
        - 85-89åˆ†ï¼šæµ‹è¯•æ–‡æ¡£ä¸­85-89%åŠŸèƒ½ç‚¹æœ‰å¯¹åº”æµ‹è¯•
        - 80-84åˆ†ï¼šæµ‹è¯•æ–‡æ¡£ä¸­80-84%åŠŸèƒ½ç‚¹æœ‰å¯¹åº”æµ‹è¯•
        - <80åˆ†ï¼šæµ‹è¯•è¦†ç›–ç‡ä½äº80%

        ### æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§è¯„åˆ†æ ‡å‡†ï¼š
        - 100åˆ†ï¼šæµ‹è¯•é€»è¾‘å®Œå…¨ç¬¦åˆä¸šåŠ¡è§„åˆ™ï¼Œæ— ä»»ä½•é€»è¾‘é”™è¯¯
        - 90-99åˆ†ï¼šæµ‹è¯•é€»è¾‘æ­£ç¡®ï¼Œä»…å­˜åœ¨æè½»å¾®ç¼ºé™·
        - 80-89åˆ†ï¼šæµ‹è¯•é€»è¾‘åŸºæœ¬æ­£ç¡®ï¼Œå­˜åœ¨è½»å¾®é€»è¾‘ç¼ºé™·
        - 70-79åˆ†ï¼šæµ‹è¯•é€»è¾‘å­˜åœ¨æ˜æ˜¾é—®é¢˜ï¼Œå¯èƒ½å½±å“æµ‹è¯•ç»“æœ
        - 60-69åˆ†ï¼šæµ‹è¯•é€»è¾‘å­˜åœ¨è¾ƒå¤šé—®é¢˜
        - <60åˆ†ï¼šæµ‹è¯•é€»è¾‘å­˜åœ¨ä¸¥é‡é”™è¯¯

        ### è¾¹ç•Œæ¡ä»¶å¤„ç†è¯„åˆ†æ ‡å‡†ï¼š
        - 100åˆ†ï¼šå……åˆ†è€ƒè™‘æ‰€æœ‰è¾¹ç•Œæ¡ä»¶ï¼ˆæå€¼ã€ä¸´ç•Œå€¼ã€å¼‚å¸¸è¾“å…¥ï¼‰
        - 90-99åˆ†ï¼šè€ƒè™‘ä¸»è¦è¾¹ç•Œæ¡ä»¶ï¼Œæå°‘é‡è¾¹ç•Œæƒ…å†µæœªè¦†ç›–
        - 80-89åˆ†ï¼šè€ƒè™‘ä¸»è¦è¾¹ç•Œæ¡ä»¶ï¼Œå°‘é‡è¾¹ç•Œæƒ…å†µæœªè¦†ç›–
        - 70-79åˆ†ï¼šè¾¹ç•Œæ¡ä»¶è€ƒè™‘ä¸è¶³ï¼Œå­˜åœ¨æ˜æ˜¾é—æ¼
        - 60-69åˆ†ï¼šè¾¹ç•Œæ¡ä»¶è€ƒè™‘ä¸¥é‡ä¸è¶³
        - <60åˆ†ï¼šå‡ ä¹æœªè€ƒè™‘è¾¹ç•Œæ¡ä»¶

        ### é”™è¯¯å¤„ç†è¯„åˆ†æ ‡å‡†ï¼š
        - 100åˆ†ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œèƒ½ä¼˜é›…å¤„ç†æ‰€æœ‰å¼‚å¸¸æƒ…å†µ
        - 90-99åˆ†ï¼šèƒ½å¤„ç†ç»å¤§éƒ¨åˆ†å¼‚å¸¸æƒ…å†µ
        - 80-89åˆ†ï¼šèƒ½å¤„ç†å¸¸è§å¼‚å¸¸æƒ…å†µ
        - 70-79åˆ†ï¼šé”™è¯¯å¤„ç†å­˜åœ¨ç¼ºé™·ï¼Œéƒ¨åˆ†å¼‚å¸¸åœºæ™¯æœªè€ƒè™‘
        - 60-69åˆ†ï¼šé”™è¯¯å¤„ç†ä¸¥é‡ä¸è¶³
        - <60åˆ†ï¼šé”™è¯¯å¤„ç†æœºåˆ¶ç¼ºå¤±

        ### ä»£ç è´¨é‡è¯„åˆ†æ ‡å‡†ï¼š
        - 100åˆ†ï¼šä»£ç ç»“æ„æ¸…æ™°ï¼Œå‘½åè§„èŒƒï¼Œæ³¨é‡Šå®Œå–„ï¼Œé«˜åº¦å¯ç»´æŠ¤
        - 90-99åˆ†ï¼šä»£ç ç»“æ„è‰¯å¥½ï¼ŒåŸºæœ¬è§„èŒƒï¼Œå¯è¯»æ€§å¾ˆå¥½
        - 80-89åˆ†ï¼šä»£ç ç»“æ„è‰¯å¥½ï¼ŒåŸºæœ¬è§„èŒƒï¼Œå¯è¯»æ€§è¾ƒå¥½
        - 70-79åˆ†ï¼šä»£ç ç»“æ„ä¸€èˆ¬ï¼Œå¯è¯»æ€§æœ‰å¾…æé«˜
        - 60-69åˆ†ï¼šä»£ç ç»“æ„è¾ƒå·®ï¼Œéš¾ä»¥ç†è§£
        - <60åˆ†ï¼šä»£ç ç»“æ„æ··ä¹±ï¼Œæ— æ³•ç»´æŠ¤

        ## è¯„ä¼°è¦æ±‚
        1. ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ‡å‡†è¯„åˆ†ï¼Œä¸è¦ä¸»è§‚åˆ¤æ–­
        2. æ¯æ¬¡è¯„åˆ†å¿…é¡»ä½¿ç”¨ç›¸åŒæ ‡å‡†
        3. å…ˆåˆ†æå†è¯„åˆ†ï¼Œç¡®ä¿è¯„åˆ†å‡†ç¡®æ€§
        4. æä¾›å…·ä½“è¯„åˆ†ä¾æ®å’Œæ”¹è¿›å»ºè®®

        è¯·è¿”å›JSONæ ¼å¼ç»“æœï¼ŒåŒ…å«å…·ä½“è¯„åˆ†ä¾æ®ã€‚
        """
    
    def _get_default_api_url(self) -> str:
        """è·å–é»˜è®¤APIåœ°å€"""
        if self.model_type == 'ollama':
            return os.getenv('API_URL', 'http://localhost:11434')
        else:  # openaiå…¼å®¹æ¥å£
            return os.getenv('API_URL', 'http://localhost:1234/v1')
    
    def _get_default_model(self) -> str:
        """è·å–é»˜è®¤æ¨¡å‹åç§°"""
        if self.model_type == 'ollama':
            return os.getenv('OLLAMA_MODEL', 'qwen3:8b')
        else:  # openaiå…¼å®¹æ¥å£
            return os.getenv('OPENAI_MODEL', 'qwen/qwen3-8b')
    
    def read_file_content(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return ""
    
    def extract_testspecs_from_md(self, md_content: str) -> List[Dict[str, str]]:
        """ä»æµ‹è¯•ç”¨ä¾‹æ–‡æ¡£æå–åŠŸèƒ½æµ‹è¯•
        
        é’ˆå¯¹æµ‹è¯•ç”¨ä¾‹æ–‡æ¡£çš„ç‰¹ç‚¹ï¼š
        1. åŒ…å«æµ‹è¯•æ­¥éª¤å’Œæ‰§è¡Œè¯´æ˜
        2. æ²¡æœ‰æ˜ç¡®çš„é¢„æœŸç»“æœåˆ—
        3. é€šè¿‡æ“ä½œæè¿°å’ŒéªŒè¯ç‚¹ä½“ç°æµ‹è¯•
        """
        import re
        testspecs = []
        
        # æå–æµ‹è¯•æ­¥éª¤è¡¨ä¸­çš„åŠŸèƒ½æè¿°
        in_test_steps = False
        lines = md_content.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # æ£€æµ‹æµ‹è¯•æ­¥éª¤è¡¨çš„å¼€å§‹
            if '|' in line and ('æµ‹è¯•æ­¥éª¤' in line or 'Test Step' in line or 'Description' in line):
                in_test_steps = True
                continue
                
            # å¤„ç†æµ‹è¯•æ­¥éª¤è¡¨ä¸­çš„è¡Œ
            if in_test_steps and '|' in line and '---' not in line:
                parts = [part.strip() for part in line.split('|') if part.strip()]
                if len(parts) >= 3:
                    timestamp = parts[0] if len(parts) >= 1 else ""
                    test_step = parts[1] if len(parts) >= 2 else ""
                    description = parts[2] if len(parts) >= 3 else ""
                    
                    # è¿‡æ»¤æ— æ•ˆè¡Œ
                    if (test_step and 
                        test_step not in ['æµ‹è¯•æ­¥éª¤', 'Test Step', 'Description'] and
                        '---' not in test_step and
                        not re.match(r'^\d+\.?\d*$', test_step) and
                        not test_step.startswith('[')):
                        
                        # æ„å»ºåŠŸèƒ½æµ‹è¯•æè¿°
                        functional_desc = self._build_functional_testspec(test_step, description)
                        
                        testspecs.append({
                            'step': test_step,
                            'expected': description,
                            'functional_testspec': functional_desc
                        })
            
            # æå–ç‹¬ç«‹çš„æµ‹è¯•æ“ä½œ
            elif re.search(r'(TS_|Set|Check|Wait)[A-Z]', line) and '|' not in line:
                # æå–æ“ä½œæè¿°
                match = re.search(r'([A-Z][a-zA-Z0-9_]+)\s*[:ï¼š]?\s*(.+)', line)
                if match:
                    operation = match.group(1)
                    description = match.group(2)
                    
                    functional_desc = self._build_functional_testspec(operation, description)
                    
                    testspecs.append({
                        'step': operation,
                        'expected': description,
                        'functional_testspec': functional_desc
                    })
        
        return testspecs
    
    def _build_functional_testspec(self, test_step: str, description: str = "") -> str:
        """æ„å»ºåŠŸèƒ½æµ‹è¯•æè¿°
        
        ä»æµ‹è¯•æ­¥éª¤å’Œæ“ä½œæè¿°ä¸­æå–æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
        """
        import re
        
        # åˆå¹¶æµ‹è¯•æ­¥éª¤å’Œæè¿°è¿›è¡Œç»Ÿä¸€åˆ†æ
        combined_text = f"{test_step} {description}".strip()
        
        # å®šä¹‰åŠŸèƒ½å…³é”®è¯æ˜ å°„
        feature_keywords = {
            'wiper_control': ['wiper', 'é›¨åˆ·', 'Wiper'],
            'speed_control': ['speed', 'é€Ÿåº¦', 'Speed'],
            'position_control': ['position', 'ä½ç½®', 'Position', 'stop'],
            'mode_control': ['intermittent', 'é—´æ­‡', 'low', 'high', 'ä½é€Ÿ', 'é«˜é€Ÿ'],
            'fault_handling': ['fault', 'failure', 'æ•…éšœ', 'é”™è¯¯', 'blocked'],
            'timing_control': ['wait', 'ç­‰å¾…', 'delay', 'å»¶æ—¶']
        }
        
        # æå–æ ¸å¿ƒåŠŸèƒ½
        features = []
        for feature, keywords in feature_keywords.items():
            for keyword in keywords:
                if keyword.lower() in combined_text.lower():
                    features.append(feature)
                    break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°åŠŸèƒ½å…³é”®è¯ï¼Œä½¿ç”¨ç®€åŒ–æè¿°
        if not features:
            # æ¸…ç†æµ‹è¯•æ­¥éª¤ï¼Œç§»é™¤TS_å‰ç¼€ç­‰
            clean_step = re.sub(r'^TS_', '', test_step)
            clean_step = re.sub(r'([A-Z])', r' \1', clean_step).strip()
            return clean_step
        
        # æ„å»ºåŠŸèƒ½æµ‹è¯•æè¿°
        feature_map = {
            'wiper_control': 'é›¨åˆ·æ§åˆ¶',
            'speed_control': 'é€Ÿåº¦æ§åˆ¶',
            'position_control': 'ä½ç½®æ§åˆ¶',
            'mode_control': 'æ¨¡å¼æ§åˆ¶',
            'fault_handling': 'æ•…éšœå¤„ç†',
            'timing_control': 'æ—¶åºæ§åˆ¶'
        }
        
        return " + ".join([feature_map.get(f, f) for f in features])
    
    def create_evaluation_prompt(self, refwritten_content: str, generated_content: str, testspecs: List[str]) -> str:
        """åˆ›å»ºAIè¯„ä¼°æç¤º"""
        
        testspecs_text = "\n".join([
            f"{i+1}. æ­¥éª¤: {req['step']} -> è¯¦ç»†æ­¥éª¤: {req['expected']}"
            for i, req in enumerate(testspecs)
        ])
        
        prompt = f"""
        è¯·ä½œä¸ºCAPLæµ‹è¯•ä¸“å®¶ï¼Œä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†è¯„ä¼°ä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹ã€‚è¯·é€é¡¹åˆ†æåå†ç»™å‡ºå‡†ç¡®è¯„åˆ†ã€‚

        ## æµ‹è¯•æ–‡æ¡£ï¼ˆå…±{len(testspecs)}é¡¹æµ‹è¯•ï¼‰
        {testspecs_text}

        ## å‚è€ƒæµ‹è¯•ç”¨ä¾‹
        ```capl
        {refwritten_content}
        ```

        ## ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        ```capl
        {generated_content}
        ```

        ## è¯„ä¼°ä»»åŠ¡
        è¯·ä¸¥æ ¼æŒ‰ç…§ä¹‹å‰å®šä¹‰çš„è¯„åˆ†æ ‡å‡†ï¼Œä»ä»¥ä¸‹6ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼š

        ### è¯„ä¼°æ­¥éª¤ï¼š
        1. **åŠŸèƒ½å®Œæ•´æ€§åˆ†æ**ï¼šå¯¹ç…§æµ‹è¯•æ–‡æ¡£ï¼Œé€ä¸€æ£€æŸ¥æ¯ä¸ªæµ‹è¯•é¡¹ç›®æ˜¯å¦è¢«æµ‹è¯•
        2. **æµ‹è¯•è¦†ç›–ç‡ç»Ÿè®¡**ï¼šè®¡ç®—è¢«æµ‹è¯•é¡¹ç›®å æ€»æµ‹è¯•çš„æ¯”ä¾‹
        3. **æµ‹è¯•é€»è¾‘éªŒè¯**ï¼šéªŒè¯æµ‹è¯•æ­¥éª¤æ˜¯å¦ç¬¦åˆä¸šåŠ¡é€»è¾‘
        4. **è¾¹ç•Œæ¡ä»¶æ£€æŸ¥**ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«è¾¹ç•Œå€¼æµ‹è¯•ï¼ˆå¦‚æå€¼ã€ä¸´ç•Œå€¼ï¼‰
        5. **é”™è¯¯å¤„ç†è¯„ä¼°**ï¼šæ£€æŸ¥å¼‚å¸¸æƒ…å†µçš„æµ‹è¯•è¦†ç›–
        6. **ä»£ç è´¨é‡è¯„ä¼°**ï¼šè¯„ä¼°ä»£ç å¯è¯»æ€§ã€ç»“æ„æ¸…æ™°åº¦

        ### è¯„åˆ†è¦æ±‚ï¼š
        - æ¯é¡¹è¯„åˆ†å¿…é¡»æœ‰æ˜ç¡®ä¾æ®
        - è¯„åˆ†å¿…é¡»ä¸ºæ•´æ•°ï¼ˆå¦‚85ï¼Œä¸æ˜¯85.5ï¼‰
        - ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†ï¼Œä¸å¾—éšæ„è°ƒæ•´

        ## è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰
        è¯·ä»¥ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
        {{
            "functional_completeness": æ•´æ•°åˆ†æ•°,
            "testspec_coverage": æ•´æ•°åˆ†æ•°,
            "test_logic_correctness": æ•´æ•°åˆ†æ•°,
            "edge_case_handling": æ•´æ•°åˆ†æ•°,
            "error_handling": æ•´æ•°åˆ†æ•°,
            "code_quality": æ•´æ•°åˆ†æ•°,
            "missing_functionalities": ["å…·ä½“ç¼ºå¤±çš„åŠŸèƒ½ç‚¹1", "å…·ä½“ç¼ºå¤±çš„åŠŸèƒ½ç‚¹2", ...],
            "redundant_tests": ["å†—ä½™æµ‹è¯•1", "å†—ä½™æµ‹è¯•2", ...],
            "improvement_suggestions": ["å…·ä½“æ”¹è¿›å»ºè®®1", "å…·ä½“æ”¹è¿›å»ºè®®2", ...],
            "detailed_analysis": "è¯¦ç»†åˆ†ææ–‡æœ¬ï¼ŒåŒ…å«è¯„åˆ†ä¾æ®",
            "scoring_basis": {{
                "functional_completeness": "è¯„åˆ†å…·ä½“ä¾æ®",
                "testspec_coverage": "è¯„åˆ†å…·ä½“ä¾æ®",
                "test_logic_correctness": "è¯„åˆ†å…·ä½“ä¾æ®",
                "edge_case_handling": "è¯„åˆ†å…·ä½“ä¾æ®",
                "error_handling": "è¯„åˆ†å…·ä½“ä¾æ®",
                "code_quality": "è¯„åˆ†å…·ä½“ä¾æ®"
            }}
        }}

        ## æ³¨æ„äº‹é¡¹
        1. å…ˆåˆ†ææ¯ä¸ªæµ‹è¯•é¡¹ç›®æ˜¯å¦è¢«æµ‹è¯•ï¼Œå†ç»™å‡ºåŠŸèƒ½å®Œæ•´æ€§è¯„åˆ†
        2. è®¡ç®—å®é™…è¦†ç›–ç‡ç™¾åˆ†æ¯”ï¼Œå†ç»™å‡ºæµ‹è¯•è¦†ç›–ç‡è¯„åˆ†
        3. æ¯é¡¹è¯„åˆ†å¿…é¡»åŸºäºå…·ä½“äº‹å®ï¼Œä¸èƒ½ä¸»è§‚åˆ¤æ–­
        4. ç¡®ä¿è¯„åˆ†çš„ä¸€è‡´æ€§ï¼ŒåŒæ ·çš„æƒ…å†µå¿…é¡»ç»™åŒæ ·çš„åˆ†æ•°
        """
        
        return prompt
    
    def call_ai_model(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨AIæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œå¢åŠ ä¸€è‡´æ€§å¤„ç†"""
        try:
            print(f"   ğŸ“¡ è°ƒç”¨AIæ¨¡å‹: {self.model_type}")
            print(f"   ğŸ¯ ç›®æ ‡æ¨¡å‹: {self.model_name}")
            print(f"   ğŸŒ¡ï¸  æ¸©åº¦å‚æ•°: {self.temperature}")
            
            # æ ¹æ®verboseå‚æ•°å†³å®šæ˜¯å¦æ‰“å°å®Œæ•´çš„promptå†…å®¹
            if self.verbose:
                print("=" * 80)
                print("ğŸ“‹ å‘é€ç»™å¤§æ¨¡å‹çš„å®Œæ•´PROMPTå†…å®¹:")
                print("=" * 80)
                print(prompt)
                print("=" * 80)
                print("ğŸ“‹ PROMPTå†…å®¹ç»“æŸ")
                print("=" * 80)
            
            # ä½¿ç”¨ä¸€è‡´æ€§ç§å­
            import hashlib
            seed = int(hashlib.md5(prompt.encode()).hexdigest(), 16) % 10000
            
            if self.model_type == "ollama":
                print(f"   ğŸ”— è¿æ¥åœ°å€: {self.api_url}")
                result = self._call_ollama(prompt)
            else:  # openaiå…¼å®¹æ¥å£
                print(f"   ğŸ”— APIåœ°å€: {self.api_url}")
                result = self._call_openai_compatible(prompt)
            
            # æ ‡å‡†åŒ–å¤„ç†ï¼Œç¡®ä¿è¯„åˆ†ä¸€è‡´æ€§
            normalized_result = self._normalize_scores(result)
            print(f"   ğŸ“Š å“åº”æ•°æ®å¤§å°: {len(str(normalized_result))} å­—ç¬¦")
            return normalized_result
            
        except Exception as e:
            print(f"AIæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return self._get_default_result()
    
    def _normalize_scores(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–è¯„åˆ†ç»“æœï¼Œç¡®ä¿ä¸€è‡´æ€§"""
        # ç¡®ä¿æ‰€æœ‰è¯„åˆ†éƒ½æ˜¯æœ‰æ•ˆçš„æ•°å€¼
        score_fields = [
            'functional_completeness', 'testspec_coverage', 
            'test_logic_correctness', 'edge_case_handling',
            'error_handling', 'code_quality'
        ]
        
        for field in score_fields:
            if field not in result or not isinstance(result[field], (int, float)):
                result[field] = 75.0  # é»˜è®¤å€¼
            else:
                # ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†…
                score = float(result[field])
                score = max(0, min(100, score))
                # æ ‡å‡†åŒ–ä¸º5çš„å€æ•°ï¼Œå‡å°‘æ³¢åŠ¨
                result[field] = round(score / 5) * 5
        
        # ç¡®ä¿åˆ—è¡¨å­—æ®µå­˜åœ¨
        for list_field in ['missing_functionalities', 'redundant_tests', 'improvement_suggestions']:
            if list_field not in result or not isinstance(result[list_field], list):
                result[list_field] = []
        
        # ç¡®ä¿è¯¦ç»†åˆ†æå­—æ®µå­˜åœ¨
        if 'detailed_analysis' not in result:
            result['detailed_analysis'] = "AIè¯„ä¼°å®Œæˆ"
            
        return result
    
    def _call_openai_compatible(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨OpenAIå…¼å®¹æ¥å£"""
        print("   â³ æ­£åœ¨è¿æ¥OpenAIå…¼å®¹æœåŠ¡...")
        # æ„å»ºå®Œæ•´çš„APIç«¯ç‚¹URL
        if self.api_url.endswith('/v1'):
            api_url = f"{self.api_url}/chat/completions"
        elif not self.api_url.endswith('/chat/completions'):
            api_url = f"{self.api_url.rstrip('/')}/chat/completions"
        else:
            api_url = self.api_url
        
        headers = {"Content-Type": "application/json"}
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        payload = {
            "model": self.model_name,
            "messages": [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt}
            ],
            "stream": False,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "max_tokens": self.max_tokens
        }
        
        try:
            print("   ğŸ“¨ å‘é€HTTPè¯·æ±‚...")
            response = requests.post(api_url, json=payload, headers=headers, timeout=120)
            response.raise_for_status()
            
            print("   âœ… æ”¶åˆ°APIå“åº”")
            data = response.json()
            content = data['choices'][0]['message']['content']
            
            # æ˜¾ç¤ºAIçš„éƒ¨åˆ†åˆ†æå†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰
            preview = content[:500] + "..." if len(content) > 500 else content
            print(f"   ğŸ§  AIåˆ†æé¢„è§ˆ: {preview}")
            
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            
            result = json.loads(content.strip())
            
            # æ˜¾ç¤ºå…³é”®åˆ†æç»“æœ
            if isinstance(result, dict):
                print(f"   ğŸ“Š åŠŸèƒ½å®Œæ•´æ€§è¯„åˆ†: {result.get('functional_completeness', 'N/A')}")
                print(f"   ğŸ“Š æµ‹è¯•è¦†ç›–ç‡è¯„åˆ†: {result.get('testspec_coverage', 'N/A')}")
                print(f"   ğŸ“Š æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§: {result.get('test_logic_correctness', 'N/A')}")
                
                missing_count = len(result.get('missing_functionalities', []))
                suggestions_count = len(result.get('improvement_suggestions', []))
                print(f"   ğŸ“‹ å‘ç°ç¼ºå¤±åŠŸèƒ½: {missing_count} é¡¹")
                print(f"   ğŸ’¡ æä¾›æ”¹è¿›å»ºè®®: {suggestions_count} æ¡")
            
            print("   âœ… JSONè§£ææˆåŠŸ")
            return result
        except requests.exceptions.ConnectionError as e:
            raise ConnectionError(f"è¿æ¥å¤±è´¥ - è¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨ (è¿è¡Œ 'ollama serve' æˆ–å¯åŠ¨ LM Studio): {e}")
        except requests.exceptions.Timeout as e:
            raise TimeoutError(f"è¯·æ±‚è¶…æ—¶ - è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒæœåŠ¡çŠ¶æ€: {e}")
        except json.JSONDecodeError as e:
            raise ValueError(f"AIå“åº”æ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            error_msg = str(e)
            if "Connection" in error_msg or "connect" in error_msg.lower():
                raise ConnectionError(f"è¿æ¥å¤±è´¥ - è¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨å¹¶æ­£åœ¨ç›‘å¬æ­£ç¡®çš„ç«¯å£: {e}")
            elif "404" in error_msg:
                raise ValueError(f"æ¨¡å‹æœªæ‰¾åˆ° - è¯·ç¡®ä¿æ¨¡å‹å·²åŠ è½½: {e}")
            else:
                raise RuntimeError(f"AIè°ƒç”¨å¤±è´¥: {e}")
    
    def _call_ollama(self, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨Ollamaæœ¬åœ°æ¨¡å‹"""
        try:
            print("   â³ æ­£åœ¨è¿æ¥OllamaæœåŠ¡...")
            ollama_host = self.api_url.rstrip('/')
            client = ollama.Client(host=ollama_host)
            
            print("   ğŸ“¨ å‘é€è¯·æ±‚åˆ°AIæ¨¡å‹...")
            response = client.chat(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": prompt}
                ],
                options={
                    "temperature": self.temperature,
                    "top_p": self.top_p,
                    "num_ctx": self.context_length,
                    "num_predict": self.max_tokens
                }
            )
            
            print("   âœ… æ”¶åˆ°AIå“åº”")
            content = response['message']['content']
            
            # æ˜¾ç¤ºAIçš„éƒ¨åˆ†åˆ†æå†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰
            preview = content[:500] + "..." if len(content) > 500 else content
            print(f"   ğŸ§  AIåˆ†æé¢„è§ˆ: {preview}")
            
            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            content = content.strip()
            if content.startswith('```json'):
                content = content[7:]
            if content.endswith('```'):
                content = content[:-3]
            
            result = json.loads(content.strip())
            
            # æ˜¾ç¤ºå…³é”®åˆ†æç»“æœ
            if isinstance(result, dict):
                print(f"   ğŸ“Š åŠŸèƒ½å®Œæ•´æ€§è¯„åˆ†: {result.get('functional_completeness', 'N/A')}")
                print(f"   ğŸ“Š æµ‹è¯•è¦†ç›–ç‡è¯„åˆ†: {result.get('testspec_coverage', 'N/A')}")
                print(f"   ğŸ“Š æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§: {result.get('test_logic_correctness', 'N/A')}")
                
                missing_count = len(result.get('missing_functionalities', []))
                suggestions_count = len(result.get('improvement_suggestions', []))
                print(f"   ğŸ“‹ å‘ç°ç¼ºå¤±åŠŸèƒ½: {missing_count} é¡¹")
                print(f"   ğŸ’¡ æä¾›æ”¹è¿›å»ºè®®: {suggestions_count} æ¡")
            
            print("   âœ… JSONè§£ææˆåŠŸ")
            return result
        except Exception as e:
            error_msg = str(e)
            if "Connection" in error_msg or "connect" in error_msg.lower():
                raise ConnectionError(f"è¿æ¥å¤±è´¥ - è¯·ç¡®ä¿OllamaæœåŠ¡å·²å¯åŠ¨ (è¿è¡Œ 'ollama serve'): {e}")
            elif "404" in error_msg:
                raise ValueError(f"æ¨¡å‹æœªæ‰¾åˆ° - è¯·è¿è¡Œ 'ollama run {self.model_name}' åŠ è½½æ¨¡å‹: {e}")
            else:
                raise RuntimeError(f"Ollamaè°ƒç”¨å¤±è´¥: {e}")
    
    def _get_default_result(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤è¯„ä¼°ç»“æœ"""
        return {
            "functional_completeness": 75.0,
            "testspec_coverage": 75.0,
            "test_logic_correctness": 75.0,
            "edge_case_handling": 70.0,
            "error_handling": 70.0,
            "code_quality": 75.0,
            "missing_functionalities": ["æ— æ³•è¿æ¥AIæ¨¡å‹è¿›è¡Œè¯¦ç»†è¯„ä¼°"],
            "redundant_tests": [],
            "improvement_suggestions": [
                "è¯·æ£€æŸ¥AIæ¨¡å‹è¿æ¥é…ç½®",
                "ç¡®ä¿Ollamaæˆ–OpenAIæœåŠ¡æ­£å¸¸è¿è¡Œ",
                "éªŒè¯ç½‘ç»œè¿æ¥å’ŒAPIå¯†é’¥"
            ],
            "detailed_analysis": "ç”±äºAIæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°ç»“æœã€‚å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®åé‡æ–°è¯„ä¼°ã€‚",
            "scoring_basis": {
                "functional_completeness": "é»˜è®¤ä¸­ç­‰è¯„åˆ†",
                "testspec_coverage": "é»˜è®¤ä¸­ç­‰è¯„åˆ†", 
                "test_logic_correctness": "é»˜è®¤ä¸­ç­‰è¯„åˆ†",
                "edge_case_handling": "é»˜è®¤ä¸­ç­‰è¯„åˆ†",
                "error_handling": "é»˜è®¤ä¸­ç­‰è¯„åˆ†",
                "code_quality": "é»˜è®¤ä¸­ç­‰è¯„åˆ†"
            }
        }
    
    def evaluate_testcase(self, testcase_id: str, refwritten_path: str, generated_path: str, testspec_path: str) -> AIEvaluationResult:
        """è¯„ä¼°å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        
        print(f"\nğŸ“‹ å¼€å§‹è¯„ä¼°æµ‹è¯•ç”¨ä¾‹ {testcase_id}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        print("ğŸ“– è¯»å–æµ‹è¯•æ–‡ä»¶...")
        refwritten_content = self.read_file_content(refwritten_path)
        generated_content = self.read_file_content(generated_path)
        testspec_content = self.read_file_content(testspec_path)
        
        if not all([refwritten_content, generated_content, testspec_content]):
            print("âŒ éƒ¨åˆ†æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯»å–")
            return AIEvaluationResult(**self._get_default_result())
        
        print(f"   âœ… å‚è€ƒæµ‹è¯•ç”¨ä¾‹: {len(refwritten_content)} å­—ç¬¦")
        print(f"   âœ… ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹: {len(generated_content)} å­—ç¬¦")
        print(f"   âœ… æµ‹è¯•æ–‡æ¡£: {len(testspec_content)} å­—ç¬¦")
        
        # æå–æµ‹è¯•
        print("ğŸ” åˆ†ææµ‹è¯•æ–‡æ¡£...")
        testspecs = self.extract_testspecs_from_md(testspec_content)
        print(f"   âœ… æå–åˆ° {len(testspecs)} ä¸ªåŠŸèƒ½æµ‹è¯•")

        # åˆ›å»ºè¯„ä¼°æç¤º
        print("ğŸ“ ç”ŸæˆAIè¯„ä¼°æç¤º...")
        prompt = self.create_evaluation_prompt(refwritten_content, generated_content, testspecs)
        prompt_size = len(prompt)
        print(f"   âœ… æç¤ºè¯é•¿åº¦: {prompt_size} å­—ç¬¦")
        
        # è°ƒç”¨AIæ¨¡å‹è¯„ä¼°
        print("ğŸ¤– è°ƒç”¨AIæ¨¡å‹...")
        ai_result = self.call_ai_model(prompt)
        print("   âœ… AIæ¨¡å‹å“åº”å®Œæˆ")
        
        # è¿‡æ»¤ç»“æœï¼Œåªä¿ç•™AIEvaluationResultæ‰€éœ€çš„å­—æ®µ
        print("ğŸ”§ å¤„ç†AIå“åº”æ•°æ®...")
        filtered_result = self._filter_ai_result(ai_result)
        print("   âœ… æ•°æ®è¿‡æ»¤å®Œæˆ")
        
        return AIEvaluationResult(**filtered_result)
    
    def generate_detailed_report(self, result: AIEvaluationResult, testcase_id: str) -> str:
        """ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""
        
        # è®¡ç®—åŠ æƒç»¼åˆè¯„åˆ†
        weighted_score = (result.functional_completeness * 0.25 + 
                         result.testspec_coverage * 0.25 + 
                         result.test_logic_correctness * 0.20 + 
                         result.edge_case_handling * 0.15 + 
                         result.error_handling * 0.10 + 
                         result.code_quality * 0.05)
        rating = self._get_rating(weighted_score)
        
        report = f"""# CAPLæµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°æŠ¥å‘Š - æµ‹è¯•ç”¨ä¾‹ {testcase_id}

## ç»¼åˆè¯„åˆ†
| è¯„ä¼°ç»´åº¦ | å¾—åˆ† | è¯„çº§ |
|----------|------|------|
| åŠŸèƒ½å®Œæ•´æ€§ | {result.functional_completeness:.1f}/100 | {self._get_rating(result.functional_completeness)} |
| æµ‹è¯•è¦†ç›–ç‡ | {result.testspec_coverage:.1f}/100 | {self._get_rating(result.testspec_coverage)} |
| æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§ | {result.test_logic_correctness:.1f}/100 | {self._get_rating(result.test_logic_correctness)} |
| è¾¹ç•Œæ¡ä»¶å¤„ç† | {result.edge_case_handling:.1f}/100 | {self._get_rating(result.edge_case_handling)} |
| é”™è¯¯å¤„ç† | {result.error_handling:.1f}/100 | {self._get_rating(result.error_handling)} |
| ä»£ç è´¨é‡ | {result.code_quality:.1f}/100 | {self._get_rating(result.code_quality)} |

## ç»¼åˆè¯„åˆ†: {weighted_score:.1f}/100 ({rating})

## è¯¦ç»†åˆ†æ
{result.detailed_analysis}

"""
        
        if result.missing_functionalities:
            report += f"""
## ç¼ºå¤±çš„åŠŸèƒ½ç‚¹
{chr(10).join([f"- {func}" for func in result.missing_functionalities])}
"""
        
        if result.redundant_tests:
            report += f"""
## å†—ä½™çš„æµ‹è¯•
{chr(10).join([f"- {test}" for test in result.redundant_tests])}
"""
        
        if result.improvement_suggestions:
            report += f"""
## æ”¹è¿›å»ºè®®
{chr(10).join([f"- {suggestion}" for suggestion in result.improvement_suggestions])}
"""
        
        return report
    
    def _filter_ai_result(self, ai_result: Dict[str, Any]) -> Dict[str, Any]:
        """è¿‡æ»¤AIå“åº”ï¼Œåªä¿ç•™AIEvaluationResultæ‰€éœ€çš„å­—æ®µ"""
        # å®šä¹‰AIEvaluationResultæ‰€éœ€çš„å­—æ®µ
        required_fields = {
            'functional_completeness', 'testspec_coverage', 'test_logic_correctness',
            'edge_case_handling', 'error_handling', 'code_quality',
            'missing_functionalities', 'redundant_tests', 'improvement_suggestions',
            'detailed_analysis', 'scoring_basis'
        }
        
        # è¿‡æ»¤å­—æ®µå¹¶è®¾ç½®é»˜è®¤å€¼
        filtered = {}
        for field in required_fields:
            if field in ai_result:
                filtered[field] = ai_result[field]
            else:
                # è®¾ç½®é»˜è®¤å€¼
                if field.endswith('_functionalities') or field.endswith('_tests') or field.endswith('_suggestions'):
                    filtered[field] = []
                elif field == 'detailed_analysis':
                    filtered[field] = "AIè¯„ä¼°å®Œæˆ"
                elif field == 'scoring_basis':
                    filtered[field] = None
                else:
                    filtered[field] = 75.0  # é»˜è®¤è¯„åˆ†
        
        # ç¡®ä¿åˆ—è¡¨å­—æ®µæ˜¯åˆ—è¡¨ç±»å‹
        list_fields = ['missing_functionalities', 'redundant_tests', 'improvement_suggestions']
        for field in list_fields:
            if not isinstance(filtered[field], list):
                filtered[field] = []
        
        return filtered
    
    def _get_rating(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è¿”å›è¯„çº§"""
        if score >= 90:
            return "ä¼˜ç§€"
        elif score >= 80:
            return "è‰¯å¥½"
        elif score >= 70:
            return "ä¸­ç­‰"
        elif score >= 60:
            return "åŠæ ¼"
        else:
            return "éœ€æ”¹è¿›"
    
    def save_evaluation_result(self, result: AIEvaluationResult, testcase_id: str, output_dir: str = "results"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå¹´æœˆæ—¥æ—¶åˆ†ç§’æ ¼å¼çš„æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        
        # ä¿å­˜JSONç»“æœ
        json_file = output_path / f"ai_evaluation_{testcase_id}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = output_path / f"ai_report_{testcase_id}_{timestamp}.md"
        report = self.generate_detailed_report(result, testcase_id)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“Š AIè¯„ä¼°ç»“æœå·²ä¿å­˜:")
        print(f"  JSONæ–‡ä»¶: {json_file}")
        print(f"  æŠ¥å‘Šæ–‡ä»¶: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='CAPLæµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°ç³»ç»Ÿ')
    parser.add_argument('testcase_id', help='æµ‹è¯•ç”¨ä¾‹ID')
    parser.add_argument('--model-type', choices=['ollama', 'openai'], default=None, help='AIæ¨¡å‹ç±»å‹ (ollama æˆ– openaiå…¼å®¹æ¥å£)')
    parser.add_argument('--api-url', help='APIæœåŠ¡åœ°å€')
    parser.add_argument('--model', help='ä½¿ç”¨çš„æ¨¡å‹åç§°')
    parser.add_argument('--context-length', type=int, help='ä¸Šä¸‹æ–‡é•¿åº¦ (tokens)')
    parser.add_argument('--max-tokens', type=int, help='æœ€å¤§è¾“å‡ºtokensæ•°')
    parser.add_argument('--temperature', type=float, help='ç”Ÿæˆæ¸©åº¦ (0.0-1.0)')
    parser.add_argument('--top-p', type=float, help='top-pé‡‡æ ·å‚æ•° (0.0-1.0)')
    
    args = parser.parse_args()
    
    # æŸ¥æ‰¾æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶
    base_dir = Path(__file__).parent.parent
    
    refwritten_path = base_dir / "test_output" / f"testcase_id_{args.testcase_id}.can"
    generated_path = base_dir / "test_output" / f"qualification_*{args.testcase_id}*.can"
    testspec_path = base_dir / "pdf_converter" / "testcases" / f"qualification_*{args.testcase_id}*.md"
    
    # æŸ¥æ‰¾å¤§æ¨¡å‹ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶
    generated_files = list(base_dir.glob(f"test_output/qualification*{args.testcase_id}*.can"))
    testspec_files = list(base_dir.glob(f"pdf_converter/testcases/qualification*{args.testcase_id}*.md"))
    
    if not generated_files:
        print(f"âŒ æœªæ‰¾åˆ°å¤§æ¨¡å‹ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶")
        return
    
    if not testspec_files:
        print(f"âŒ æœªæ‰¾åˆ°æµ‹è¯•æ–‡æ¡£")
        return
    
    generated_path = generated_files[0]
    testspec_path = testspec_files[0]
    
    if not refwritten_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°å‚è€ƒæµ‹è¯•ç”¨ä¾‹: {refwritten_path}")
        return
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CAPLAIEvaluator(
        model_type=args.model_type,
        api_url=args.api_url,
        model_name=args.model
    )
    
    # æ‰§è¡Œè¯„ä¼°
    print(f"ğŸ¤– å¼€å§‹AIè¯„ä¼°æµ‹è¯•ç”¨ä¾‹ {args.testcase_id}...")
    result = evaluator.evaluate_testcase(
        args.testcase_id,
        str(refwritten_path),
        str(generated_path),
        str(testspec_path)
    )
    
    # ä¿å­˜ç»“æœ
    evaluator.save_evaluation_result(result, args.testcase_id)
    
    # æ‰“å°ç®€è¦ç»“æœ
    print(f"\nğŸ“Š AIè¯„ä¼°å®Œæˆ!")
    print(f"åŠŸèƒ½å®Œæ•´æ€§: {result.functional_completeness:.1f}/100")
    print(f"æµ‹è¯•è¦†ç›–ç‡: {result.testspec_coverage:.1f}/100")
    print(f"æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§: {result.test_logic_correctness:.1f}/100")

if __name__ == "__main__":
    main()