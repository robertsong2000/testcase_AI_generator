# CAPLæµ‹è¯•ç”¨ä¾‹AIæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ - ä½¿ç”¨æŒ‡å—

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

è¿™ä¸ªAIè¯„ä¼°ç³»ç»Ÿä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ä»**åŠŸèƒ½å®Œæ•´æ€§**è§’åº¦è¯„ä¼°CAPLæµ‹è¯•ç”¨ä¾‹ï¼Œè€Œéç®€å•çš„å­—ç¬¦ä¸²ç›¸ä¼¼åº¦æ¯”è¾ƒã€‚å®ƒèƒ½ï¼š

- âœ… è¯„ä¼°åŠŸèƒ½éœ€æ±‚è¦†ç›–åº¦
- âœ… åˆ†ææµ‹è¯•é€»è¾‘æ­£ç¡®æ€§
- âœ… è¯†åˆ«ç¼ºå¤±çš„æµ‹è¯•åœºæ™¯
- âœ… æ£€æµ‹å†—ä½™æµ‹è¯•ç”¨ä¾‹
- âœ… æä¾›å…·ä½“æ”¹è¿›å»ºè®®

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install ollama requests python-dotenv
```

### 2. é…ç½®æ–‡ä»¶è®¾ç½®

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ç°æœ‰é…ç½®æ–‡ä»¶
é¡¹ç›®æ ¹ç›®å½•å·²æä¾›é…ç½®æ–‡ä»¶æ¨¡æ¿ï¼š
```bash
# ä½¿ç”¨Ollamaé…ç½®
cp .env.ollama.sample .env

# æˆ–ä½¿ç”¨OpenAIé…ç½®
cp .env.openai.sample .env
```

#### æ–¹æ³•äºŒï¼šæ‰‹åŠ¨åˆ›å»ºé…ç½®æ–‡ä»¶
åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
# APIç±»å‹: ollama æˆ– openai
API_TYPE=ollama

# APIæœåŠ¡åœ°å€
API_URL=http://localhost:11434

# æ¨¡å‹åç§°
OLLAMA_MODEL=qwen3:8b
OPENAI_MODEL=qwen/qwen3-8b

# OpenAI APIå¯†é’¥ï¼ˆå¦‚ä½¿ç”¨OpenAIï¼‰
OPENAI_API_KEY=sk-xxx
```

### 3. ä½¿ç”¨æ–¹æ³•

#### åŸºç¡€ç”¨æ³•
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆOllama + qwen3:8bï¼‰
python run_ai_evaluation.py 1336732

# æŒ‡å®šæµ‹è¯•ç”¨ä¾‹IDå’Œæ¨¡å‹
python run_ai_evaluation.py 1336732 --model-type ollama --model qwen3:8b

# ä½¿ç”¨OpenAI
python run_ai_evaluation.py 1336732 --model-type openai --api-key sk-xxx --model gpt-4

# ä½¿ç”¨LM Studio
python run_ai_evaluation.py 1336732 --model-type openai --api-url http://localhost:1234/v1 --model qwen/qwen3-8b
```

#### é«˜çº§å‚æ•°
```bash
# è‡ªå®šä¹‰æ¸©åº¦å’Œtokené™åˆ¶
python run_ai_evaluation.py 1336732 --temperature 0.2 --max-tokens 4000

# ä½¿ç”¨ä¸åŒçš„æç¤ºè¯æ¨¡æ¿
python run_ai_evaluation.py 1336732 --prompt-template detailed

# å¯ç”¨è°ƒè¯•æ¨¡å¼
python run_ai_evaluation.py 1336732 --debug
```

## ğŸ“Š è¯„ä¼°ç»´åº¦

### æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡

| ç»´åº¦ | æƒé‡ | è¯„åˆ†æ ‡å‡† | ä¼˜ç§€æ ‡å‡† |
|------|------|----------|----------|
| **åŠŸèƒ½å®Œæ•´æ€§** | 25% | æ˜¯å¦è¦†ç›–æ‰€æœ‰åŠŸèƒ½éœ€æ±‚ | â‰¥85åˆ† |
| **éœ€æ±‚è¦†ç›–ç‡** | 25% | éœ€æ±‚æ–‡æ¡£åŠŸèƒ½ç‚¹è¦†ç›–ç¨‹åº¦ | â‰¥90åˆ† |
| **æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§** | 20% | æµ‹è¯•é€»è¾‘æ˜¯å¦ç¬¦åˆä¸šåŠ¡è§„åˆ™ | â‰¥80åˆ† |
| **è¾¹ç•Œæ¡ä»¶å¤„ç†** | 15% | è¾¹ç•Œå€¼å’Œå¼‚å¸¸æƒ…å†µè€ƒè™‘ | â‰¥75åˆ† |
| **é”™è¯¯å¤„ç†** | 10% | é”™è¯¯æƒ…å†µå¤„ç†å®Œå–„ç¨‹åº¦ | â‰¥70åˆ† |
| **ä»£ç è´¨é‡** | 5% | å¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§ | â‰¥80åˆ† |

### è¯„ä¼°ç­‰çº§è¯´æ˜

- **ä¼˜ç§€ (90-100åˆ†)**: æµ‹è¯•ç”¨ä¾‹å®Œæ•´è¦†ç›–éœ€æ±‚ï¼Œé€»è¾‘ä¸¥è°¨ï¼Œè¾¹ç•Œæ¡ä»¶å……åˆ†è€ƒè™‘
- **è‰¯å¥½ (80-89åˆ†)**: åŸºæœ¬è¦†ç›–éœ€æ±‚ï¼Œé€»è¾‘æ­£ç¡®ï¼Œä¸»è¦è¾¹ç•Œæ¡ä»¶å·²è€ƒè™‘
- **ä¸€èˆ¬ (70-79åˆ†)**: éƒ¨åˆ†éœ€æ±‚æœªè¦†ç›–ï¼Œé€»è¾‘å­˜åœ¨å°é—®é¢˜ï¼Œè¾¹ç•Œæ¡ä»¶è€ƒè™‘ä¸è¶³
- **éœ€æ”¹è¿› (<70åˆ†)**: éœ€æ±‚è¦†ç›–ä¸å®Œæ•´ï¼Œé€»è¾‘æœ‰æ˜æ˜¾ç¼ºé™·ï¼Œéœ€å¤§å¹…ä¿®æ”¹

## ğŸ“ è¾“å‡ºæ–‡ä»¶

è¿è¡Œåä¼šç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

### 1. JSONç»“æœæ–‡ä»¶
**è·¯å¾„**: `results/ai_evaluation_{testcase_id}_{timestamp}.json`
**å†…å®¹**:
- 6é¡¹è¯„ä¼°æŒ‡æ ‡å¾—åˆ†
- ç¼ºå¤±åŠŸèƒ½ç‚¹åˆ—è¡¨
- æ”¹è¿›å»ºè®®
- è¯¦ç»†å¯¹æ¯”æ•°æ®

### 2. è¯¦ç»†æŠ¥å‘Š
**è·¯å¾„**: `results/ai_report_{testcase_id}_{timestamp}.md`
**å†…å®¹**:
- è¯„ä¼°æ‘˜è¦
- ç¼ºå¤±åŠŸèƒ½åˆ†æ
- å…·ä½“æ”¹è¿›å»ºè®®
- æµ‹è¯•ç”¨ä¾‹å¯¹æ¯”

### 3. ç¤ºä¾‹è¾“å‡º
```
ğŸ“Š AIè¯„ä¼°å®Œæˆ! æµ‹è¯•ç”¨ä¾‹: 1336732
========================================
ç»¼åˆè¯„åˆ†: 80.75/100 (è‰¯å¥½)
åŠŸèƒ½å®Œæ•´æ€§: 85.0/100
éœ€æ±‚è¦†ç›–ç‡: 90.0/100
æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§: 75.0/100
è¾¹ç•Œæ¡ä»¶å¤„ç†: 65.0/100
é”™è¯¯å¤„ç†: 70.0/100
ä»£ç è´¨é‡: 85.0/100
========================================

âš ï¸ ç¼ºå¤±åŠŸèƒ½ç‚¹ (3):
  - æœªæµ‹è¯•é›¨åˆ·åœ¨æä½æ¸©åº¦ä¸‹çš„å·¥ä½œæƒ…å†µ
  - ç¼ºå°‘ç”µæºæ³¢åŠ¨æ—¶çš„ç¨³å®šæ€§æµ‹è¯•
  - æœªéªŒè¯è¿ç»­å·¥ä½œçŠ¶æ€ä¸‹çš„æ€§èƒ½

ğŸ’¡ ä¸»è¦æ”¹è¿›å»ºè®® (3):
  - å¢åŠ ä½æ¸©ç¯å¢ƒæµ‹è¯•ç”¨ä¾‹ (-40Â°C)
  - æ·»åŠ ç”µæºç”µå‹å˜åŒ–æµ‹è¯•åœºæ™¯ (9V-16V)
  - è¡¥å……é•¿æ—¶é—´è¿è¡Œå‹åŠ›æµ‹è¯• (è¿ç»­å·¥ä½œ4å°æ—¶)
```

## ğŸ” è¯„ä¼°ç»“æœç¤ºä¾‹

### å®é™…è¯„ä¼°ç»“æœç¤ºä¾‹

```json
{
  "testcase_id": "1336732",
  "overall_score": 80.75,
  "scores": {
    "functional_completeness": 85.0,
    "requirement_coverage": 90.0,
    "test_logic_correctness": 75.0,
    "boundary_condition_handling": 65.0,
    "error_handling": 70.0,
    "code_quality": 85.0
  },
  "missing_features": [
    "æœªæµ‹è¯•é›¨åˆ·åœ¨æä½æ¸©åº¦ä¸‹çš„å·¥ä½œæƒ…å†µ",
    "ç¼ºå°‘ç”µæºæ³¢åŠ¨æ—¶çš„ç¨³å®šæ€§æµ‹è¯•",
    "æœªéªŒè¯è¿ç»­å·¥ä½œçŠ¶æ€ä¸‹çš„æ€§èƒ½"
  ],
  "improvement_suggestions": [
    "å¢åŠ ä½æ¸©ç¯å¢ƒæµ‹è¯•ç”¨ä¾‹ (-40Â°C)",
    "æ·»åŠ ç”µæºç”µå‹å˜åŒ–æµ‹è¯•åœºæ™¯ (9V-16V)",
    "è¡¥å……é•¿æ—¶é—´è¿è¡Œå‹åŠ›æµ‹è¯• (è¿ç»­å·¥ä½œ4å°æ—¶)"
  ],
  "analysis": {
    "strengths": [
      "éœ€æ±‚è¦†ç›–ç‡ä¼˜ç§€ (90.0/100)",
      "ä»£ç è´¨é‡è‰¯å¥½ (85.0/100)",
      "åŠŸèƒ½å®Œæ•´æ€§è¾ƒå¥½ (85.0/100)"
    ],
    "weaknesses": [
      "è¾¹ç•Œæ¡ä»¶å¤„ç†ä¸è¶³ (65.0/100)",
      "é”™è¯¯å¤„ç†éœ€è¦æ”¹è¿› (70.0/100)",
      "æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§å¾…æå‡ (75.0/100)"
    ]
  }
}
```

### è¯¦ç»†æŠ¥å‘Šç¤ºä¾‹

#### è¯„ä¼°æ‘˜è¦
æµ‹è¯•ç”¨ä¾‹1336732çš„ç»¼åˆè¯„åˆ†ä¸º**80.75/100**ï¼Œå±äº**è‰¯å¥½**ç­‰çº§ã€‚

#### è¯¦ç»†åˆ†æ

**å¼ºé¡¹åˆ†æï¼š**
- âœ… **éœ€æ±‚è¦†ç›–ç‡**: 90.0/100 - åŸºæœ¬è¦†ç›–äº†æ‰€æœ‰åŠŸèƒ½éœ€æ±‚
- âœ… **ä»£ç è´¨é‡**: 85.0/100 - ä»£ç ç»“æ„æ¸…æ™°ï¼Œå‘½åè§„èŒƒ
- âœ… **åŠŸèƒ½å®Œæ•´æ€§**: 85.0/100 - ä¸»è¦åŠŸèƒ½æµ‹è¯•å®Œæ•´

**å¾…æ”¹è¿›é¡¹ï¼š**
- âš ï¸ **è¾¹ç•Œæ¡ä»¶å¤„ç†**: 65.0/100 - ç¼ºå°‘æç«¯æ¡ä»¶æµ‹è¯•
- âš ï¸ **é”™è¯¯å¤„ç†**: 70.0/100 - å¼‚å¸¸åœºæ™¯è¦†ç›–ä¸è¶³
- âš ï¸ **æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§**: 75.0/100 - éƒ¨åˆ†æµ‹è¯•é€»è¾‘éœ€è¦éªŒè¯

#### å…·ä½“æ”¹è¿›å»ºè®®

1. **è¾¹ç•Œæ¡ä»¶ä¼˜åŒ–** (ä¼˜å…ˆçº§ï¼šé«˜)
   - æ·»åŠ ä½æ¸©æµ‹è¯•ï¼š-40Â°Cç¯å¢ƒæµ‹è¯•
   - æ·»åŠ é«˜æ¸©æµ‹è¯•ï¼š85Â°Cç¯å¢ƒæµ‹è¯•
   - æ·»åŠ ç”µå‹è¾¹ç•Œï¼š9V-16Vç”µæºæ³¢åŠ¨æµ‹è¯•

2. **é”™è¯¯å¤„ç†å¢å¼º** (ä¼˜å…ˆçº§ï¼šä¸­)
   - æ·»åŠ ä¼ æ„Ÿå™¨æ•…éšœæ¨¡æ‹Ÿ
   - æ·»åŠ é€šä¿¡ä¸­æ–­æµ‹è¯•
   - æ·»åŠ æ‰§è¡Œå™¨æ•…éšœæµ‹è¯•

3. **æµ‹è¯•é€»è¾‘å®Œå–„** (ä¼˜å…ˆçº§ï¼šä¸­)
   - éªŒè¯æµ‹è¯•æ­¥éª¤çš„æ­£ç¡®æ€§
   - ç¡®ä¿é¢„æœŸç»“æœä¸å®é™…éœ€æ±‚åŒ¹é…
   - æ·»åŠ çŠ¶æ€è½¬æ¢éªŒè¯

#### ä¼˜åŒ–åé¢„æœŸå¾—åˆ†

å®Œæˆä¸Šè¿°æ”¹è¿›åï¼Œé¢„è®¡ç»¼åˆè¯„åˆ†å¯æå‡è‡³**88-93åˆ†**åŒºé—´ã€‚

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿

å¯ä»¥è‡ªå®šä¹‰è¯„ä¼°æç¤ºè¯æ¨¡æ¿ï¼š

```python
from evaluation.ai_evaluator import CAPLAIEvaluator

# åˆ›å»ºè‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
CUSTOM_PROMPT = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„CAPLæµ‹è¯•å·¥ç¨‹å¸ˆï¼Œè¯·è¯„ä¼°ä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹çš„è´¨é‡ã€‚

**è¯„ä¼°æ ‡å‡†ï¼š**
- åŠŸèƒ½å®Œæ•´æ€§ï¼šæ˜¯å¦è¦†ç›–æ‰€æœ‰éœ€æ±‚ç‚¹
- éœ€æ±‚è¦†ç›–ç‡ï¼šéœ€æ±‚æ–‡æ¡£ä¸­çš„åŠŸèƒ½ç‚¹æ˜¯å¦éƒ½è¢«æµ‹è¯•
- æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§ï¼šæµ‹è¯•æ­¥éª¤æ˜¯å¦ç¬¦åˆé€»è¾‘
- è¾¹ç•Œæ¡ä»¶å¤„ç†ï¼šæ˜¯å¦æµ‹è¯•äº†è¾¹ç•Œå€¼å’Œå¼‚å¸¸æƒ…å†µ
- é”™è¯¯å¤„ç†ï¼šæ˜¯å¦å¤„ç†äº†å¯èƒ½çš„é”™è¯¯åœºæ™¯
- ä»£ç è´¨é‡ï¼šä»£ç çš„å¯è¯»æ€§å’Œè§„èŒƒæ€§

**æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯ï¼š**
- æµ‹è¯•ç”¨ä¾‹ID: {testcase_id}
- åŠŸèƒ½éœ€æ±‚: {requirements}

**æ‰‹å†™æµ‹è¯•ç”¨ä¾‹:**
```capl
{handwritten}
```

**AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹:**
```capl
{generated}
```

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
    "overall_score": <0-100>,
    "functional_completeness": <0-100>,
    "requirement_coverage": <0-100>,
    "test_logic_correctness": <0-100>,
    "boundary_condition_handling": <0-100>,
    "error_handling": <0-100>,
    "code_quality": <0-100>,
    "missing_features": ["..."],
    "improvement_suggestions": ["..."],
    "detailed_analysis": "..."
}}
"""

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡æ¿
evaluator = CAPLAIEvaluator(prompt_template=CUSTOM_PROMPT)
```

### 2. æ‰¹é‡è¯„ä¼°å¤šä¸ªæµ‹è¯•ç”¨ä¾‹

#### åŸºç¡€æ‰¹é‡è¯„ä¼°
```python
import os
import json
from evaluation.ai_evaluator import CAPLAIEvaluator

def batch_evaluate_testcases(testcase_ids, model_type="ollama"):
    """æ‰¹é‡è¯„ä¼°æµ‹è¯•ç”¨ä¾‹"""
    evaluator = CAPLAIEvaluator(model_type=model_type)
    results = []
    
    for testcase_id in testcase_ids:
        try:
            result = evaluator.evaluate_testcase(
                testcase_id=testcase_id,
                handwritten_path=f"test_output/testcase_id_{testcase_id}.can",
                generated_path=f"test_output/qualification_{testcase_id}.can",
                requirement_path=f"pdf_converter/testcases/qualification_{testcase_id}.md"
            )
            
            results.append({
                "testcase_id": testcase_id,
                "result": result,
                "status": "success"
            })
            
            print(f"âœ… {testcase_id}: {result['overall_score']}/100")
            
        except Exception as e:
            results.append({
                "testcase_id": testcase_id,
                "error": str(e),
                "status": "failed"
            })
            print(f"âŒ {testcase_id}: å¤±è´¥ - {str(e)}")
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
testcase_ids = ["1336732", "1336763", "1336764"]
results = batch_evaluate_testcases(testcase_ids)
```

#### å¸¦ç»Ÿè®¡åˆ†æçš„æ‰¹é‡è¯„ä¼°
```python
import pandas as pd
import matplotlib.pyplot as plt

def analyze_evaluation_results(results):
    """åˆ†ææ‰¹é‡è¯„ä¼°ç»“æœ"""
    
    # æå–æˆåŠŸè¯„ä¼°çš„ç»“æœ
    successful_results = [r for r in results if r['status'] == 'success']
    
    if not successful_results:
        print("æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„ç»“æœ")
        return
    
    # åˆ›å»ºDataFrame
    scores_data = []
    for r in successful_results:
        scores = r['result']['scores']
        scores['testcase_id'] = r['testcase_id']
        scores_data.append(scores)
    
    df = pd.DataFrame(scores_data)
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("=== è¯„ä¼°ç»Ÿè®¡ ===")
    print(f"æ€»æµ‹è¯•ç”¨ä¾‹: {len(results)}")
    print(f"æˆåŠŸè¯„ä¼°: {len(successful_results)}")
    print(f"å¤±è´¥è¯„ä¼°: {len(results) - len(successful_results)}")
    print()
    
    print("=== å¾—åˆ†ç»Ÿè®¡ ===")
    print(df.describe())
    print()
    
    # æ‰¾å‡ºéœ€è¦æ”¹è¿›çš„æµ‹è¯•ç”¨ä¾‹
    low_scores = df[df['overall_score'] < 75]
    if not low_scores.empty:
        print("=== éœ€è¦æ”¹è¿›çš„æµ‹è¯•ç”¨ä¾‹ ===")
        for _, row in low_scores.iterrows():
            print(f"{row['testcase_id']}: {row['overall_score']}/100")
    
    return df

# ä½¿ç”¨ç¤ºä¾‹
df = analyze_evaluation_results(results)
```

### 3. æ€§èƒ½ä¼˜åŒ–

#### å¹¶è¡Œè¯„ä¼°ï¼ˆéœ€è¦å®‰è£…joblibï¼‰
```python
from joblib import Parallel, delayed
from evaluation.ai_evaluator import CAPLAIEvaluator

def parallel_evaluate(testcase_id):
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹è¯„ä¼°å‡½æ•°"""
    try:
        evaluator = CAPLAIEvaluator()
        result = evaluator.evaluate_testcase(
            testcase_id=testcase_id,
            handwritten_path=f"test_output/testcase_id_{testcase_id}.can",
            generated_path=f"test_output/qualification_{testcase_id}.can",
            requirement_path=f"pdf_converter/testcases/qualification_{testcase_id}.md"
        )
        return {
            "testcase_id": testcase_id,
            "result": result,
            "status": "success"
        }
    except Exception as e:
        return {
            "testcase_id": testcase_id,
            "error": str(e),
            "status": "failed"
        }

# å¹¶è¡Œè¯„ä¼°
testcase_ids = ["1336732", "1336763", "1336764"]
results = Parallel(n_jobs=3)(delayed(parallel_evaluate)(tid) for tid in testcase_ids)
```

### 4. ç»“æœå¯è§†åŒ–

#### ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šå›¾è¡¨
```python
import matplotlib.pyplot as plt
import seaborn as sns

def create_evaluation_report(df):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šå›¾è¡¨"""
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. æ•´ä½“å¾—åˆ†åˆ†å¸ƒ
    axes[0,0].hist(df['overall_score'], bins=10, edgecolor='black', alpha=0.7)
    axes[0,0].set_title('æ•´ä½“å¾—åˆ†åˆ†å¸ƒ')
    axes[0,0].set_xlabel('å¾—åˆ†')
    axes[0,0].set_ylabel('æµ‹è¯•ç”¨ä¾‹æ•°é‡')
    
    # 2. å„æŒ‡æ ‡å¹³å‡åˆ†
    metrics = ['functional_completeness', 'requirement_coverage', 'test_logic_correctness', 
               'boundary_condition_handling', 'error_handling', 'code_quality']
    avg_scores = df[metrics].mean()
    
    axes[0,1].bar(range(len(metrics)), avg_scores.values)
    axes[0,1].set_title('å„æŒ‡æ ‡å¹³å‡åˆ†')
    axes[0,1].set_xticks(range(len(metrics)))
    axes[0,1].set_xticklabels([m.replace('_', '\n') for m in metrics], rotation=45)
    axes[0,1].set_ylim(0, 100)
    
    # 3. æµ‹è¯•ç”¨ä¾‹å¾—åˆ†æ’å
    df_sorted = df.sort_values('overall_score', ascending=False)
    axes[1,0].barh(df_sorted['testcase_id'], df_sorted['overall_score'])
    axes[1,0].set_title('æµ‹è¯•ç”¨ä¾‹å¾—åˆ†æ’å')
    axes[1,0].set_xlabel('å¾—åˆ†')
    
    # 4. çƒ­åŠ›å›¾
    sns.heatmap(df[metrics].corr(), annot=True, fmt='.2f', ax=axes[1,1])
    axes[1,1].set_title('æŒ‡æ ‡ç›¸å…³æ€§çƒ­åŠ›å›¾')
    
    plt.tight_layout()
    plt.savefig('evaluation_report.png', dpi=300, bbox_inches='tight')
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
create_evaluation_report(df)
```

## ğŸ¯ ä¸ä¼ ç»Ÿæ¯”è¾ƒçš„åŒºåˆ«

| ä¼ ç»Ÿå­—ç¬¦ä¸²æ¯”è¾ƒ | AIæ™ºèƒ½è¯„ä¼° |
|----------------|------------|
| åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦ | åŸºäºåŠŸèƒ½ç†è§£ |
| è¡¨é¢å½¢å¼æ¯”è¾ƒ | æ·±å±‚é€»è¾‘åˆ†æ |
| æ— æ³•è¯†åˆ«ä¸šåŠ¡é€»è¾‘ | ç†è§£æµ‹è¯•ç›®çš„ |
| ç»™å‡ºæ¨¡ç³Šè¯„åˆ† | æä¾›å…·ä½“å»ºè®® |
| å¿½ç•¥ä¸šåŠ¡åœºæ™¯ | è€ƒè™‘å®é™…éœ€æ±‚ |

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **APIé…é¢**: OpenAIæœ‰è°ƒç”¨é¢‘ç‡é™åˆ¶ï¼Œå¤§é‡è¯„ä¼°è¯·ä½¿ç”¨æœ¬åœ°Ollama
2. **ç½‘ç»œè¿æ¥**: ä½¿ç”¨OpenAIéœ€è¦ç¨³å®šç½‘ç»œï¼ŒOllamaå¯ç¦»çº¿è¿è¡Œ
3. **æ¨¡å‹é€‰æ‹©**: 
   - GPT-4æ›´å‡†ç¡®ä½†æˆæœ¬é«˜
   - Llama2å…è´¹ä½†ç²¾åº¦ç•¥ä½
4. **ç»“æœè§£é‡Š**: AIè¯„ä¼°æ˜¯è¾…åŠ©å·¥å…·ï¼Œæœ€ç»ˆå†³ç­–ä»éœ€äººå·¥ç¡®è®¤

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: Ollamaè¿æ¥å¤±è´¥**
```bash
# æ£€æŸ¥OllamaæœåŠ¡
ollama serve

# é‡æ–°æ‹‰å–æ¨¡å‹
ollama pull llama2
```

**Q: OpenAI APIé”™è¯¯**
```bash
# æ£€æŸ¥APIå¯†é’¥
export OPENAI_API_KEY="your-valid-key"

# æ£€æŸ¥é…é¢
curl https://api.openai.com/v1/models -H "Authorization: Bearer $OPENAI_API_KEY"
```

**Q: æ–‡ä»¶æ‰¾ä¸åˆ°**
```bash
# æ£€æŸ¥æ–‡ä»¶è·¯å¾„
ls test_output/
ls pdf_converter/testcases/
```

### 1. APIè¿æ¥å¤±è´¥
**é—®é¢˜æè¿°**: æ— æ³•è¿æ¥åˆ°AIæ¨¡å‹API

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
ollama ps

# å¦‚æœæœåŠ¡æœªå¯åŠ¨ï¼Œå¯åŠ¨Ollama
ollama serve

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
ollama list

# å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œæ‹‰å–æ¨¡å‹
ollama pull qwen3:8b
```

**Pythonè°ƒè¯•**:
```python
import requests

# æµ‹è¯•Ollamaè¿æ¥
try:
    response = requests.get("http://localhost:11434/api/tags")
    print("Ollamaè¿æ¥æ­£å¸¸")
    print("å¯ç”¨æ¨¡å‹:", response.json())
except Exception as e:
    print("Ollamaè¿æ¥å¤±è´¥:", e)
```

### 2. è¯„ä¼°ç»“æœå¼‚å¸¸
**é—®é¢˜æè¿°**: è¯„ä¼°ç»“æœä¸å‡†ç¡®æˆ–å¼‚å¸¸

**æ’æŸ¥æ­¥éª¤**:
1. **æ£€æŸ¥è¾“å…¥æ–‡ä»¶**:
   ```bash
   # éªŒè¯æ–‡ä»¶å­˜åœ¨
   ls -la test_output/testcase_id_1336732.can
   ls -la test_output/qualification_1336732.can
   ls -la pdf_converter/testcases/qualification_1336732.md
   
   # æ£€æŸ¥æ–‡ä»¶å†…å®¹
   head -20 test_output/testcase_id_1336732.can
   ```

2. **éªŒè¯æµ‹è¯•ç”¨ä¾‹æ ¼å¼**:
   ```python
   def validate_testcase_files(testcase_id):
       files = [
           f"test_output/testcase_id_{testcase_id}.can",
           f"test_output/qualification_{testcase_id}.can",
           f"pdf_converter/testcases/qualification_{testcase_id}.md"
       ]
       
       for file_path in files:
           if not os.path.exists(file_path):
               print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
               return False
           
           if os.path.getsize(file_path) == 0:
               print(f"âŒ æ–‡ä»¶ä¸ºç©º: {file_path}")
               return False
       
       print("âœ… æ‰€æœ‰æ–‡ä»¶éªŒè¯é€šè¿‡")
       return True
   ```

### 3. æ€§èƒ½é—®é¢˜
**é—®é¢˜æè¿°**: è¯„ä¼°è¿‡ç¨‹ç¼“æ…¢æˆ–è¶…æ—¶

**ä¼˜åŒ–æ–¹æ¡ˆ**:

1. **ä½¿ç”¨æœ¬åœ°æ¨¡å‹**:
   ```python
   # æ¨èä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹
   evaluator = CAPLAIEvaluator(
       model_type="ollama",
       model_name="qwen3:8b",
       timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
   )
   ```

2. **è°ƒæ•´å¹¶å‘æ•°**:
   ```python
   # å‡å°‘å¹¶è¡Œä»»åŠ¡æ•°
   from joblib import Parallel, delayed
   results = Parallel(n_jobs=2)(delayed(evaluate_single)(tid) for tid in testcase_ids)
   ```

3. **ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°**:
   ```python
   def batch_evaluate_with_size_limit(testcase_ids, batch_size=5):
       """åˆ†æ‰¹è¯„ä¼°ï¼Œé¿å…ä¸€æ¬¡å¤„ç†è¿‡å¤š"""
       results = []
       for i in range(0, len(testcase_ids), batch_size):
           batch = testcase_ids[i:i+batch_size]
           batch_results = batch_evaluate_testcases(batch)
           results.extend(batch_results)
           print(f"å®Œæˆæ‰¹æ¬¡ {i//batch_size + 1}/{(len(testcase_ids)-1)//batch_size + 1}")
       return results
   ```

### 4. å†…å­˜é—®é¢˜
**é—®é¢˜æè¿°**: å†…å­˜ä¸è¶³æˆ–ç¨‹åºå´©æºƒ

**è§£å†³æ–¹æ¡ˆ**:
```python
import gc

def memory_safe_evaluate(testcase_ids):
    """å†…å­˜å®‰å…¨çš„è¯„ä¼°æ–¹å¼"""
    results = []
    
    for testcase_id in testcase_ids:
        # æ¯æ¬¡åˆ›å»ºæ–°çš„è¯„ä¼°å™¨å®ä¾‹
        evaluator = CAPLAIEvaluator()
        
        try:
            result = evaluator.evaluate_testcase(
                testcase_id=testcase_id,
                handwritten_path=f"test_output/testcase_id_{testcase_id}.can",
                generated_path=f"test_output/qualification_{testcase_id}.can",
                requirement_path=f"pdf_converter/testcases/qualification_{testcase_id}.md"
            )
            results.append(result)
            
        except Exception as e:
            print(f"è¯„ä¼° {testcase_id} æ—¶å‡ºé”™: {e}")
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        del evaluator
        gc.collect()
    
    return results
```

### 5. è°ƒè¯•æ¨¡å¼

**å¯ç”¨è°ƒè¯•æ—¥å¿—**:
```python
import logging

# è®¾ç½®è°ƒè¯•æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('evaluation_debug.log'),
        logging.StreamHandler()
    ]
)

# åœ¨è°ƒè¯•æ¨¡å¼ä¸‹è¿è¡Œè¯„ä¼°
evaluator = CAPLAIEvaluator(debug=True)
result = evaluator.evaluate_testcase("1336732")
```

### 6. é”™è¯¯æ¢å¤

**è‡ªåŠ¨é‡è¯•æœºåˆ¶**:
```python
import time
from functools import wraps

def retry_on_failure(max_attempts=3, delay=5):
    """å¤±è´¥é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise e
                    print(f"å°è¯• {attempt + 1} å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯•...")
                    time.sleep(delay)
        return wrapper
    return decorator

@retry_on_failure(max_attempts=3, delay=10)
def safe_evaluate(testcase_id):
    evaluator = CAPLAIEvaluator()
    return evaluator.evaluate_testcase(testcase_id)
```

## ğŸš€ ä¸‹ä¸€æ­¥

### å³å°†æ¨å‡ºçš„åŠŸèƒ½

#### 1. å¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°
- æ”¯æŒåŒæ—¶å¯¹æ¯”å¤šä¸ªAIæ¨¡å‹çš„è¯„ä¼°ç»“æœ
- æä¾›æ¨¡å‹æ€§èƒ½æ’è¡Œæ¦œ
- è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹

#### 2. æ™ºèƒ½æ”¹è¿›å»ºè®®
- åŸºäºè¯„ä¼°ç»“æœè‡ªåŠ¨ç”Ÿæˆæ”¹è¿›ä»£ç 
- ä¸€é”®åº”ç”¨æ”¹è¿›å»ºè®®
- æ”¹è¿›æ•ˆæœè¿½è¸ª

#### 3. å®æ—¶ç›‘æ§é¢æ¿
- Webç•Œé¢å®æ—¶æŸ¥çœ‹è¯„ä¼°è¿›åº¦
- äº¤äº’å¼ç»“æœåˆ†æ
- å†å²æ•°æ®è¶‹åŠ¿å›¾

#### 4. å›¢é˜Ÿåä½œåŠŸèƒ½
- è¯„ä¼°ç»“æœå…±äº«
- è¯„è®ºå’Œè®¨è®ºåŠŸèƒ½
- æ”¹è¿›å»ºè®®æŠ•ç¥¨ç³»ç»Ÿ

### å‚ä¸å¼€å‘

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›AIè¯„ä¼°ç³»ç»Ÿï¼

#### å¼€å‘è·¯çº¿å›¾
- **v3.1**: å¤šæ¨¡å‹å¯¹æ¯”åŠŸèƒ½
- **v3.2**: æ™ºèƒ½ä»£ç ä¿®å¤
- **v3.3**: Webç•Œé¢
- **v3.4**: å›¢é˜Ÿåä½œåŠŸèƒ½

#### å¦‚ä½•è´¡çŒ®
1. Forké¡¹ç›®ä»£ç 
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ”¹è¿›ä»£ç 
4. åˆ›å»ºPull Request

### è”ç³»æˆ‘ä»¬

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- æäº¤GitHub Issue
- å‘é€é‚®ä»¶è‡³å¼€å‘å›¢é˜Ÿ
- åŠ å…¥ç¤¾åŒºè®¨è®ºç¾¤ç»„

---

**æœ€åæ›´æ–°æ—¶é—´**: 2024å¹´12æœˆ20æ—¥  
**æ–‡æ¡£ç‰ˆæœ¬**: v3.0  
**ç³»ç»Ÿç‰ˆæœ¬**: AIæ™ºèƒ½è¯„ä¼°ç³»ç»Ÿ v3.0