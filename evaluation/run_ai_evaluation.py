#!/usr/bin/env python3
"""
AIè¯„ä¼°ç³»ç»Ÿè¿è¡Œå™¨
ç®€åŒ–ç‰ˆå‘½ä»¤è¡Œå·¥å…·ï¼Œå¿«é€Ÿè¿è¡ŒAIè¯„ä¼°
"""

import os
import sys
import time
from pathlib import Path
import argparse
import json
from ai_evaluator import CAPLAIEvaluator

def find_testcase_files(testcase_id: str) -> dict:
    """æŸ¥æ‰¾æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶"""
    base_dir = Path(__file__).parent.parent
    
    # æ‰‹å†™æµ‹è¯•ç”¨ä¾‹
    handwritten = base_dir / "test_output" / f"testcase_id_{testcase_id}.can"
    
    # ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹
    generated_files = list(base_dir.glob(f"test_output/qualification*{testcase_id}*.can"))
    generated = generated_files[0] if generated_files else None
    
    # éœ€æ±‚æ–‡æ¡£
    requirement_files = list(base_dir.glob(f"pdf_converter/testcases/qualification*{testcase_id}*.md"))
    requirement = requirement_files[0] if requirement_files else None
    
    return {
        'handwritten': handwritten,
        'generated': generated,
        'requirement': requirement
    }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='CAPLæµ‹è¯•ç”¨ä¾‹AIè¯„ä¼°')
    parser.add_argument('testcase_id', help='æµ‹è¯•ç”¨ä¾‹IDï¼Œå¦‚ 1336732')
    parser.add_argument('--model-type', choices=['ollama', 'openai'], default=None,
                       help='AIæ¨¡å‹ç±»å‹ (ollama æˆ– openaiå…¼å®¹æ¥å£)')
    parser.add_argument('--api-url', help='APIæœåŠ¡åœ°å€')
    parser.add_argument('--model', help='ä½¿ç”¨çš„æ¨¡å‹åç§°')
    parser.add_argument('--api-key', help='APIå¯†é’¥ (å¯é€‰)')
    parser.add_argument('--temperature', type=float, default=0.05, 
                       help='æ¨¡å‹æ¸©åº¦å‚æ•°ï¼Œè¶Šä½è¶Šä¸€è‡´ (é»˜è®¤: 0.05)')
    parser.add_argument('--consistent-mode', action='store_true',
                       help='å¯ç”¨ä¸€è‡´æ€§æ¨¡å¼ï¼Œç¡®ä¿è¯„åˆ†ç¨³å®š')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†æ¨¡å¼ï¼Œæ‰“å°å‘é€ç»™AIçš„å®Œæ•´promptå†…å®¹')
    
    args = parser.parse_args()
    
    # æŸ¥æ‰¾æ–‡ä»¶
    files = find_testcase_files(args.testcase_id)
    
    # æ£€æŸ¥æ–‡ä»¶
    missing = []
    for key, file in files.items():
        if not file or not file.exists():
            missing.append(key)
    
    if missing:
        print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {', '.join(missing)}")
        print("æ‰¾åˆ°çš„æ–‡ä»¶:")
        for key, file in files.items():
            if file and file.exists():
                print(f"  âœ… {key}: {file.name}")
            else:
                print(f"  âŒ {key}: æœªæ‰¾åˆ°")
        return
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CAPLAIEvaluator(
        model_type=args.model_type,
        api_url=args.api_url,
        model_name=args.model,
        api_key=args.api_key,
        verbose=args.verbose
    )
    
    # è®¾ç½®æ¸©åº¦å‚æ•°ä»¥æé«˜ä¸€è‡´æ€§
    if args.temperature is not None:
        evaluator.temperature = args.temperature
    
    if args.consistent_mode:
        evaluator.temperature = 0.01  # æä½æ¸©åº¦ç¡®ä¿æœ€å¤§ä¸€è‡´æ€§
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"ğŸ¤– å¼€å§‹AIè¯„ä¼°æµ‹è¯•ç”¨ä¾‹ {args.testcase_id}...")
    print(f"æ‰‹å†™æ–‡ä»¶: {files['handwritten'].name}")
    print(f"ç”Ÿæˆæ–‡ä»¶: {files['generated'].name}")
    print(f"éœ€æ±‚æ–‡ä»¶: {files['requirement'].name}")
    print(f"AIé…ç½®: ä½¿ç”¨{evaluator.model_type}æ¨¡å‹ ({evaluator.model_name})")
    
    # æ‰§è¡Œè¯„ä¼°ï¼ˆå¸¦è¯¦ç»†è¿‡ç¨‹è¾“å‡ºï¼‰
    print(f"\nğŸ”„ å¼€å§‹AIåˆ†æè¿‡ç¨‹...")
    print("-" * 50)
    
    start_time = time.time()
    
    print(f"\nğŸ¤– è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ...")
    print(f"   æ¨¡å‹: {evaluator.model_name}")
    print(f"   æ¸©åº¦: {evaluator.temperature}")
    
    result = evaluator.evaluate_testcase(
        args.testcase_id,
        str(files['handwritten']),
        str(files['generated']),
        str(files['requirement'])
    )
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
    evaluator.save_evaluation_result(result, args.testcase_id)
    print(f"   âœ… ç»“æœå·²ä¿å­˜åˆ° evaluation/results/ ç›®å½•")
    print(f"   â±ï¸  æ€»è€—æ—¶: {elapsed_time:.1f}ç§’")
    
    # è®¡ç®—åŠ æƒç»¼åˆè¯„åˆ†
    weighted_score = (result.functional_completeness * 0.25 + 
                     result.requirement_coverage * 0.25 + 
                     result.test_logic_correctness * 0.20 + 
                     result.edge_case_handling * 0.15 + 
                     result.error_handling * 0.10 + 
                     result.code_quality * 0.05)
    
    # è·å–è¯„çº§
    def get_rating(score):
        if score >= 90:
            return "ä¼˜ç§€"
        elif score >= 80:
            return "è‰¯å¥½"
        elif score >= 70:
            return "ä¸€èˆ¬"
        else:
            return "éœ€æ”¹è¿›"
    
    # ç®€è¦ç»“æœ
    rating = get_rating(weighted_score)
    print(f"\nğŸ“Š AIè¯„ä¼°å®Œæˆ!")
    print(f"=" * 50)
    print(f"åŠŸèƒ½å®Œæ•´æ€§: {result.functional_completeness:.1f}/100")
    print(f"éœ€æ±‚è¦†ç›–ç‡: {result.requirement_coverage:.1f}/100")
    print(f"æµ‹è¯•é€»è¾‘æ­£ç¡®æ€§: {result.test_logic_correctness:.1f}/100")
    print(f"è¾¹ç•Œæ¡ä»¶å¤„ç†: {result.edge_case_handling:.1f}/100")
    print(f"é”™è¯¯å¤„ç†: {result.error_handling:.1f}/100")
    print(f"ä»£ç è´¨é‡: {result.code_quality:.1f}/100")
    print(f"-" * 50)
    print(f"ç»¼åˆè¯„åˆ†: {weighted_score:.1f}/100 ({rating})")
    print(f"=" * 50)
    
    if result.missing_functionalities:
        print(f"\nâš ï¸ ç¼ºå¤±åŠŸèƒ½ç‚¹ ({len(result.missing_functionalities)}):")
        for func in result.missing_functionalities:
            print(f"  - {func}")
    
    if result.improvement_suggestions:
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(result.improvement_suggestions)}):")
        for suggestion in result.improvement_suggestions:
            print(f"  - {suggestion}")

if __name__ == "__main__":
    main()