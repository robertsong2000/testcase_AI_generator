#!/usr/bin/env python3
"""
æµ‹è¯•ç”¨ä¾‹LLMå¢å¼ºå™¨
åŸºäºLLM+RAGçš„æµ‹è¯•ç”¨ä¾‹å¢å¼ºç³»ç»Ÿ
"""

import json
from pathlib import Path
from typing import Dict, Any, Optional, List

from capl_langchain.factories.llm_factory import LLMFactory
from capl_langchain.managers.knowledge_manager import KnowledgeManager


class TestcaseLLMEnhancer:
    """
    åŸºäºLLM+RAGçš„æµ‹è¯•ç”¨ä¾‹å¢å¼ºå™¨
    
    åŠŸèƒ½:
    - æ™ºèƒ½å¢å¼ºæµ‹è¯•ç”¨ä¾‹æè¿°
    - åŸºäºçŸ¥è¯†åº“ä¼˜åŒ–æµ‹è¯•æ­¥éª¤
    - ç”Ÿæˆæ›´è¯¦ç»†çš„é¢„æœŸç»“æœ
    """
    
    def __init__(self, config, verbose: bool = False):
        """
        åˆå§‹åŒ–å¢å¼ºå™¨
        
        Args:
            config: EnhancerConfigå¯¹è±¡ï¼ŒåŒ…å«LLMå’ŒRAGç›¸å…³é…ç½®
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        """
        self.config = config
        self.verbose = verbose
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = LLMFactory.create_llm(config)
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨
        self.knowledge_manager = KnowledgeManager(config)
    
    def enhance_testcase(self, testcase_path: str, step_index: Optional[int] = None) -> Optional[Dict[str, Any]]:
        """
        å¢å¼ºæµ‹è¯•ç”¨ä¾‹
        
        Args:
            testcase_path: æµ‹è¯•ç”¨ä¾‹æ–‡ä»¶è·¯å¾„
            step_index: æŒ‡å®šè¦å¢å¼ºçš„æ­¥éª¤ç´¢å¼•ï¼ŒNoneè¡¨ç¤ºå¢å¼ºæ‰€æœ‰æ­¥éª¤
            
        Returns:
            å¢å¼ºåçš„æµ‹è¯•ç”¨ä¾‹æ•°æ®
        """
        try:
            if self.verbose:
                print(f"ğŸš€ å¼€å§‹å¢å¼ºæµ‹è¯•ç”¨ä¾‹: {testcase_path}")
            
            # åŠ è½½æµ‹è¯•ç”¨ä¾‹
            with open(testcase_path, 'r', encoding='utf-8') as f:
                testcase = json.load(f)
            
            # å¢å¼ºæµ‹è¯•ç”¨ä¾‹
            enhanced = self._enhance_testcase(testcase, step_index)
            
            if self.verbose:
                print("âœ… æµ‹è¯•ç”¨ä¾‹å¢å¼ºå®Œæˆ")
            
            return enhanced
            
        except Exception as e:
            if self.verbose:
                print(f"âŒ å¢å¼ºæµ‹è¯•ç”¨ä¾‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def _enhance_testcase(self, testcase: Dict[str, Any], step_index: Optional[int] = None) -> Dict[str, Any]:
        """
        å¢å¼ºæµ‹è¯•ç”¨ä¾‹çš„æ ¸å¿ƒé€»è¾‘
        
        Args:
            testcase: åŸå§‹æµ‹è¯•ç”¨ä¾‹
            step_index: æŒ‡å®šæ­¥éª¤ç´¢å¼•
            
        Returns:
            å¢å¼ºåçš„æµ‹è¯•ç”¨ä¾‹
        """
        enhanced = testcase.copy()
        
        # å¢å¼ºæµ‹è¯•ç”¨ä¾‹æè¿°
        if 'description' in testcase:
            enhanced['description'] = self._enhance_description_with_llm(
                testcase.get('description', ''),
                testcase.get('purpose', ''),
                testcase.get('preconditions', [])
            )
        
        # å¢å¼ºæ­¥éª¤
        if 'steps' in testcase:
            steps = testcase['steps']
            
            if step_index is not None:
                # å¢å¼ºæŒ‡å®šæ­¥éª¤
                if 0 <= step_index < len(steps):
                    enhanced['steps'][step_index] = self._enhance_step(
                        steps[step_index], 
                        step_index,
                        testcase
                    )
            else:
                # å¢å¼ºæ‰€æœ‰æ­¥éª¤
                enhanced_steps = []
                for i, step in enumerate(steps):
                    if self.verbose:
                        print(f"ğŸ”„ æ­£åœ¨å¢å¼ºæ­¥éª¤ {i+1}/{len(steps)}...")
                    
                    enhanced_step = self._enhance_step(step, i, testcase)
                    enhanced_steps.append(enhanced_step)
                
                enhanced['steps'] = enhanced_steps
        
        return enhanced
    
    def _analyze_testcase_purpose(self, testcase: Dict[str, Any]) -> str:
        """
        åˆ†ææµ‹è¯•ç”¨ä¾‹çš„ä¸»è¦ç›®çš„
        
        Args:
            testcase: æµ‹è¯•ç”¨ä¾‹
            
        Returns:
            æµ‹è¯•ç”¨ä¾‹ç›®çš„çš„æ–‡æœ¬æè¿°
        """
        purpose = testcase.get('purpose', '')
        if not purpose:
            # ä»æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯æ¨æ–­ç›®çš„
            title = testcase.get('title', '')
            description = testcase.get('description', '')
            
            prompt = f"""
            åŸºäºä»¥ä¸‹æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´çš„æµ‹è¯•ç›®çš„æè¿°ï¼š
            
            æ ‡é¢˜: {title}
            æè¿°: {description}
            
            è¯·ç”¨ä¸€å¥è¯æ€»ç»“è¿™ä¸ªæµ‹è¯•ç”¨ä¾‹çš„ä¸»è¦ç›®çš„ã€‚
            """
            
            try:
                response = self.llm_client.invoke(prompt)
                purpose = response.content if hasattr(response, 'content') else str(response)
            except Exception as e:
                if self.verbose:
                    print(f"âš ï¸ åˆ†ææµ‹è¯•ç›®çš„å¤±è´¥: {e}")
                purpose = f"æµ‹è¯•{title}åŠŸèƒ½"
        
        return purpose
    
    def _enhance_step(self, step: Dict[str, Any], step_index: int, testcase: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¢å¼ºå•ä¸ªæµ‹è¯•æ­¥éª¤
        
        Args:
            step: åŸå§‹æ­¥éª¤
            step_index: æ­¥éª¤ç´¢å¼•
            testcase: å®Œæ•´æµ‹è¯•ç”¨ä¾‹
            
        Returns:
            å¢å¼ºåçš„æ­¥éª¤
        """
        enhanced_step = step.copy()
        
        # æ„å»ºæ­¥éª¤ä¸Šä¸‹æ–‡
        context = self._build_step_context(step_index, testcase)
        
        # å¢å¼ºæ­¥éª¤æè¿°
        if 'description' in step:
            original_description = step['description']
            enhanced_description = self._call_llm_for_step(
                original_description,
                context,
                step_index
            )
            
            if enhanced_description and enhanced_description != original_description:
                enhanced_step['description'] = enhanced_description
                enhanced_step['original_description'] = original_description
        
        # å¢å¼ºé¢„æœŸç»“æœ
        if 'expected_result' in step:
            original_expected = step['expected_result']
            enhanced_expected = self._call_llm_for_expected_result(
                original_expected,
                enhanced_step.get('description', step.get('description', '')),
                context
            )
            
            if enhanced_expected and enhanced_expected != original_expected:
                enhanced_step['expected_result'] = enhanced_expected
                enhanced_step['original_expected_result'] = original_expected
        
        return enhanced_step
    
    def _build_step_context(self, step_index: int, testcase: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ„å»ºæ­¥éª¤ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            step_index: å½“å‰æ­¥éª¤ç´¢å¼•
            testcase: æµ‹è¯•ç”¨ä¾‹
            
        Returns:
            ä¸Šä¸‹æ–‡ä¿¡æ¯å­—å…¸
        """
        context = {
            'testcase_title': testcase.get('title', ''),
            'testcase_purpose': self._analyze_testcase_purpose(testcase),
            'preconditions': testcase.get('preconditions', []),
            'current_step_index': step_index + 1,
            'total_steps': len(testcase.get('steps', [])),
            'previous_steps': [],
            'next_steps': []
        }
        
        # æ·»åŠ å‰åæ­¥éª¤ä¿¡æ¯
        steps = testcase.get('steps', [])
        if step_index > 0:
            context['previous_steps'] = steps[:step_index]
        if step_index < len(steps) - 1:
            context['next_steps'] = steps[step_index + 1:]
        
        return context
    
    def _enhance_description_with_llm(self, original_description: str, purpose: str, preconditions: List[str]) -> str:
        """
        ä½¿ç”¨LLMå¢å¼ºæµ‹è¯•ç”¨ä¾‹æè¿°
        
        Args:
            original_description: åŸå§‹æè¿°
            purpose: æµ‹è¯•ç›®çš„
            preconditions: å‰ç½®æ¡ä»¶
            
        Returns:
            å¢å¼ºåçš„æè¿°
        """
        try:
            # æŸ¥è¯¢ç›¸å…³çŸ¥è¯†
            knowledge_context = ""
            if self.config.enable_rag:
                try:
                    knowledge_context = self.knowledge_manager.search_documents(
                        query=f"{purpose} {original_description}",
                        k=3
                    )
                except Exception as e:
                    if self.verbose:
                        print(f"âš ï¸ çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {e}")
            
            prompt = f"""
            åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œè¯·å¢å¼ºè¿™ä¸ªæµ‹è¯•ç”¨ä¾‹çš„æè¿°ï¼Œä½¿å…¶æ›´æ¸…æ™°ã€è¯¦ç»†ï¼š
            
            åŸå§‹æè¿°: {original_description}
            æµ‹è¯•ç›®çš„: {purpose}
            å‰ç½®æ¡ä»¶: {', '.join(preconditions)}
            
            ç›¸å…³èƒŒæ™¯çŸ¥è¯†:
            {knowledge_context}
            
            è¦æ±‚:
            1. ä¿æŒåŸæœ‰æ ¸å¿ƒä¿¡æ¯ä¸å˜
            2. å¢åŠ å¿…è¦çš„ç»†èŠ‚å’Œè§£é‡Š
            3. ä½¿ç”¨æ›´ä¸“ä¸šçš„æŠ€æœ¯è¯­è¨€
            4. æ§åˆ¶åœ¨{self.config.max_description_length}å­—ä»¥å†…
            
            è¯·ç›´æ¥è¿”å›å¢å¼ºåçš„æè¿°ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚
            """
            
            response = self.llm_client.invoke(prompt)
            enhanced_description = response.content if hasattr(response, 'content') else str(response)
            
            return enhanced_description.strip()
            
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ æè¿°å¢å¼ºå¤±è´¥: {e}")
            return original_description
    
    def _call_llm_for_step(self, original_description: str, context: Dict[str, Any], step_index: int) -> str:
        """
        è°ƒç”¨LLMå¢å¼ºæµ‹è¯•æ­¥éª¤
        
        Args:
            original_description: åŸå§‹æ­¥éª¤æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            step_index: æ­¥éª¤ç´¢å¼•
            
        Returns:
            å¢å¼ºåçš„æ­¥éª¤æè¿°
        """
        try:
            # æŸ¥è¯¢ç›¸å…³çŸ¥è¯†
            knowledge_context = ""
            if self.config.enable_rag:
                try:
                    knowledge_context = self.knowledge_manager.search_documents(
                        query=f"{context['testcase_title']} {original_description}",
                        k=2
                    )
                except Exception as e:
                    if self.verbose:
                        print(f"âš ï¸ çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {e}")
            
            prompt = f"""
            åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œè¯·å¢å¼ºè¿™ä¸ªæµ‹è¯•æ­¥éª¤çš„æè¿°ï¼Œä½¿å…¶æ›´æ¸…æ™°ã€è¯¦ç»†ï¼š
            
            æµ‹è¯•ç”¨ä¾‹: {context['testcase_title']}
            æµ‹è¯•ç›®çš„: {context['testcase_purpose']}
            å½“å‰æ­¥éª¤: ç¬¬{step_index + 1}æ­¥ï¼Œå…±{context['total_steps']}æ­¥
            åŸå§‹æè¿°: {original_description}
            
            å‰ç½®æ¡ä»¶: {', '.join(context['preconditions'])}
            
            ç›¸å…³èƒŒæ™¯çŸ¥è¯†:
            {knowledge_context}
            
            è¦æ±‚:
            1. ä¿æŒåŸæœ‰æ ¸å¿ƒæ“ä½œä¸å˜
            2. å¢åŠ å¿…è¦çš„ç»†èŠ‚å’Œè§£é‡Š
            3. ä½¿ç”¨æ›´ä¸“ä¸šçš„æŠ€æœ¯è¯­è¨€
            4. æ˜ç¡®æŒ‡å‡ºæ“ä½œçš„å…·ä½“ç›®çš„å’Œé¢„æœŸè¡Œä¸º
            5. æ§åˆ¶åœ¨{self.config.max_description_length}å­—ä»¥å†…
            
            è¯·ç›´æ¥è¿”å›å¢å¼ºåçš„æ­¥éª¤æè¿°ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚
            """
            
            response = self.llm_client.invoke(prompt)
            enhanced_description = response.content if hasattr(response, 'content') else str(response)
            
            return enhanced_description.strip()
            
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ æ­¥éª¤å¢å¼ºå¤±è´¥: {e}")
            return original_description
    
    def _call_llm_for_expected_result(self, original_expected: str, step_description: str, context: Dict[str, Any]) -> str:
        """
        è°ƒç”¨LLMå¢å¼ºé¢„æœŸç»“æœ
        
        Args:
            original_expected: åŸå§‹é¢„æœŸç»“æœ
            step_description: æ­¥éª¤æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            å¢å¼ºåçš„é¢„æœŸç»“æœ
        """
        try:
            prompt = f"""
            åŸºäºä»¥ä¸‹æµ‹è¯•æ­¥éª¤ï¼Œè¯·å¢å¼ºå…¶é¢„æœŸç»“æœï¼Œä½¿å…¶æ›´è¯¦ç»†ã€å…·ä½“ï¼š
            
            æµ‹è¯•ç”¨ä¾‹: {context['testcase_title']}
            æ­¥éª¤æè¿°: {step_description}
            åŸå§‹é¢„æœŸç»“æœ: {original_expected}
            
            è¦æ±‚:
            1. ä¿æŒåŸæœ‰æ ¸å¿ƒé¢„æœŸä¸å˜
            2. å¢åŠ å…·ä½“çš„éªŒè¯æ ‡å‡†å’Œæ£€æŸ¥ç‚¹
            3. æ˜ç¡®æŒ‡å‡ºç³»ç»Ÿåº”è¯¥è¡¨ç°å‡ºçš„å…·ä½“è¡Œä¸º
            4. ä½¿ç”¨æ›´ä¸“ä¸šçš„æŠ€æœ¯è¯­è¨€
            5. æ§åˆ¶åœ¨{self.config.max_description_length}å­—ä»¥å†…
            
            è¯·ç›´æ¥è¿”å›å¢å¼ºåçš„é¢„æœŸç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚
            """
            
            response = self.llm_client.invoke(prompt)
            enhanced_expected = response.content if hasattr(response, 'content') else str(response)
            
            return enhanced_expected.strip()
            
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸ é¢„æœŸç»“æœå¢å¼ºå¤±è´¥: {e}")
            return original_expected