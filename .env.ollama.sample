# Ollama API 配置
API_TYPE=ollama
API_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:30b-a3b
EMBEDDING_MODEL=nomic-embed-text

# 上下文长度配置 (tokens)
# 默认: 2048, 推荐: 4096-8192, 最大取决于模型支持
# 注意: 更大的上下文会消耗更多内存和计算时间
OLLAMA_CONTEXT_LENGTH=8192

# 最大输出长度配置 (tokens)
# 用于防止模型陷入无限循环，限制单次生成的最大token数
# 推荐: 2048-4096, 根据代码复杂度调整
OLLAMA_MAX_TOKENS=4096

# 模型采样参数配置
# TEMPERATURE: 控制输出随机性，值越高结果越随机
# TOP_P: 控制核采样，值越高结果越多样化
# 推荐: TEMPERATURE=0.2, TOP_P=0.5
TEMPERATURE=0.2
TOP_P=0.5

# RAG配置参数
# RAG_K: 检索返回的文档数量 (默认: 6 - 优化用于复杂测试用例)
# CHUNK_SIZE: 文档分块大小，单位字符 (默认: 400 - 高精度场景)
# CHUNK_OVERLAP: 文档分块重叠大小，单位字符 (默认: 50 - 高精度场景)
# 优化建议: 
# - 高精度场景: CHUNK_SIZE=400, CHUNK_OVERLAP=50
# - 平衡场景: CHUNK_SIZE=600, CHUNK_OVERLAP=100
# - 完整上下文: CHUNK_SIZE=800, CHUNK_OVERLAP=150
RAG_K=6
CHUNK_SIZE=400
CHUNK_OVERLAP=50