# Ollama API 配置
API_TYPE=ollama
API_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:30b-a3b
EMBEDDING_MODEL=nomic-embed-text

# 上下文长度配置 (tokens)
# 默认: 2048, 推荐: 4096-8192, 最大取决于模型支持
# 注意: 更大的上下文会消耗更多内存和计算时间
OLLAMA_CONTEXT_LENGTH=8192

# 最大输出长度配置 (tokens)
# 用于防止模型陷入无限循环，限制单次生成的最大token数
# 推荐: 2048-4096, 根据代码复杂度调整
OLLAMA_MAX_TOKENS=4096

# 模型采样参数配置
# TEMPERATURE: 控制输出随机性，值越高结果越随机
# TOP_P: 控制核采样，值越高结果越多样化
# 推荐: TEMPERATURE=0.2, TOP_P=0.5
TEMPERATURE=0.2
TOP_P=0.5