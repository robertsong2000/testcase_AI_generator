"""
ç»Ÿä¸€çŸ¥è¯†åº“ç®¡ç†å™¨ï¼Œæ”¯æŒå¯é…ç½®çš„æ··åˆæ£€ç´¢
"""

import os
from pathlib import Path
from typing import Any, Dict, List, Optional

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.document_loaders.json_loader import JSONLoader
try:
    from langchain_chroma import Chroma
except ImportError:
    from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

from ..config.config import CAPLGeneratorConfig
from ..factories.embedding_factory import EmbeddingFactory

# å¯é€‰çš„æ··åˆæ£€ç´¢æ”¯æŒ
try:
    from ..utils.hybrid_search import LightweightHybridSearch
    HYBRID_SEARCH_AVAILABLE = True
except ImportError:
    LightweightHybridSearch = None
    HYBRID_SEARCH_AVAILABLE = False

# å¯¼å…¥é‡æ’åºå™¨
try:
    from ..utils.result_reranker import ResultReranker
except ImportError:
    ResultReranker = None


class UnifiedKnowledgeBaseManager:
    """ç»Ÿä¸€çŸ¥è¯†åº“ç®¡ç†å™¨ï¼Œæ”¯æŒæ··åˆæ£€ç´¢å’Œçº¯å‘é‡æ£€ç´¢"""
    
    def __init__(self, config: CAPLGeneratorConfig):
        self.config = config
        self.vector_store = None
        self.hybrid_search = None
        self.reranker = None
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        self._init_reranker()
        
        # æ ¹æ®é…ç½®åˆå§‹åŒ–æ··åˆæ£€ç´¢
        if config.use_hybrid_search and HYBRID_SEARCH_AVAILABLE:
            print("ğŸ”„ æ··åˆæ£€ç´¢å·²å¯ç”¨")
        elif config.use_hybrid_search and not HYBRID_SEARCH_AVAILABLE:
            print("âš ï¸ æ··åˆæ£€ç´¢ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨çº¯å‘é‡æ£€ç´¢")
    
    def _init_reranker(self):
        """åˆå§‹åŒ–é‡æ’åºå™¨"""
        if ResultReranker is not None:
            # è·å–APIæ–‡ä»¶è·¯å¾„
            api_files = []
            if hasattr(self.config, 'api_files') and self.config.api_files:
                api_files = self.config.api_files
            else:
                # ä½¿ç”¨é»˜è®¤çš„APIæ–‡ä»¶
                kb_dir = Path(self.config.knowledge_base_dir)
                api_files = [
                    str(kb_dir / "interfaces_analysis_common-libraries.json"),
                    str(kb_dir / "interfaces_analysis_libraries.json")
                ]
        
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜å…ˆçº§æ˜ å°„æ–‡ä»¶
            priority_mapping_file = None
            if hasattr(self.config, 'api_priority_mapping_file') and self.config.api_priority_mapping_file:
                if Path(self.config.api_priority_mapping_file).exists():
                    priority_mapping_file = str(self.config.api_priority_mapping_file)
                    print(f"ğŸ“Š ä½¿ç”¨APIä¼˜å…ˆçº§æ˜ å°„æ–‡ä»¶: {priority_mapping_file}")
            
            # åªä½¿ç”¨å­˜åœ¨çš„APIæ–‡ä»¶
            valid_api_files = [f for f in api_files if Path(f).exists()]
            if priority_mapping_file:
                # ä½¿ç”¨ä¼˜å…ˆçº§æ˜ å°„æ–‡ä»¶
                self.reranker = ResultReranker(
                    api_files=valid_api_files,
                    priority_mapping_file=priority_mapping_file
                )
            elif valid_api_files:
                # ä½¿ç”¨APIæ–‡ä»¶
                self.reranker = ResultReranker(api_files=valid_api_files)
            else:
                # ä½¿ç”¨ç©ºAPIæ–‡ä»¶åˆ—è¡¨åˆ›å»ºé‡æ’åºå™¨ï¼ˆä½¿ç”¨é»˜è®¤è¡Œä¸ºï¼‰
                self.reranker = ResultReranker(api_files=[])
    
    def initialize_knowledge_base(self) -> bool:
        """åˆå§‹åŒ–çŸ¥è¯†åº“ï¼Œæ”¯æŒæ··åˆæ£€ç´¢é…ç½®"""
        if not self.config.enable_rag:
            return False
            
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.config.knowledge_base_dir.mkdir(exist_ok=True)
            self.config.vector_db_dir.mkdir(exist_ok=True)
            
            # æ£€æŸ¥ç¼“å­˜
            if self._is_cache_valid():
                print("ğŸ“¦ å‘ç°æœ‰æ•ˆç¼“å­˜ï¼Œè·³è¿‡çŸ¥è¯†åº“åˆå§‹åŒ–...")
                return self._load_from_cache()
            
            if self.config.use_hybrid_search:
                print("ğŸ”„ åˆå§‹åŒ–çŸ¥è¯†åº“å’Œæ··åˆæ£€ç´¢å™¨...")
            else:
                print("ğŸ”„ åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ˆçº¯å‘é‡æ£€ç´¢ï¼‰...")
            
            # åŠ è½½æ–‡æ¡£
            documents = self._load_documents()
            if not documents:
                print("è­¦å‘Š: çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
                return False
            
            # åˆ†å‰²æ–‡æ¡£
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap,
                length_function=len
            )
            splits = text_splitter.split_documents(documents)
            
            # åˆ›å»ºåµŒå…¥æ¨¡å‹
            embeddings = EmbeddingFactory.create_embeddings(self.config)
            
            # ä¿®å¤å†…å®¹æ ¼å¼
            for doc in splits:
                if hasattr(doc, 'page_content'):
                    if isinstance(doc.page_content, (list, dict)):
                        doc.page_content = str(doc.page_content)
                    doc.page_content = str(doc.page_content)
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            try:
                from langchain_chroma import Chroma
                self.vector_store = Chroma.from_documents(
                    documents=splits,
                    embedding=embeddings,
                    persist_directory=str(self.config.vector_db_dir)
                )
            except ImportError:
                from langchain_community.vectorstores import Chroma
                self.vector_store = Chroma.from_documents(
                    documents=splits,
                    embedding=embeddings,
                    persist_directory=str(self.config.vector_db_dir)
                )
            
            # å¦‚æœéœ€è¦æ··åˆæ£€ç´¢ä¸”å¯ç”¨ï¼Œåˆ›å»ºæ··åˆæ£€ç´¢å™¨
            if self.config.use_hybrid_search and HYBRID_SEARCH_AVAILABLE:
                self.hybrid_search = LightweightHybridSearch(
                    vector_store=self.vector_store,
                    embeddings=embeddings,
                    vector_weight=0.7,
                    keyword_weight=0.3
                )
                print("âœ… æ··åˆæ£€ç´¢å™¨å·²åˆ›å»º")
            
            # åˆ›å»ºç¼“å­˜æ ‡è®°
            self._create_cache_marker()
            
            print(f"âœ… çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ")
            print(f"   ğŸ“Š æ–‡æ¡£æ•°é‡: {len(documents)}")
            print(f"   ğŸ§© æ–‡æœ¬å—æ•°é‡: {len(splits)}")
            if self.hybrid_search:
                print(f"   ğŸ” æ··åˆæ£€ç´¢å™¨å·²å°±ç»ª")
            else:
                print(f"   ğŸ“Š å‘é‡æ£€ç´¢å™¨å·²å°±ç»ª")
            
            return True
            
        except Exception as e:
            print(f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _load_from_cache(self) -> bool:
        """ä»ç¼“å­˜åŠ è½½"""
        try:
            embeddings = EmbeddingFactory.create_embeddings(self.config)
            
            # åŠ è½½å‘é‡å­˜å‚¨
            try:
                from langchain_chroma import Chroma
                self.vector_store = Chroma(
                    persist_directory=str(self.config.vector_db_dir),
                    embedding_function=embeddings
                )
            except ImportError:
                from langchain_community.vectorstores import Chroma
                self.vector_store = Chroma(
                    persist_directory=str(self.config.vector_db_dir),
                    embedding_function=embeddings
                )
            
            # å¦‚æœéœ€è¦æ··åˆæ£€ç´¢ä¸”å¯ç”¨ï¼Œåˆ›å»ºæ··åˆæ£€ç´¢å™¨
            if self.config.use_hybrid_search and HYBRID_SEARCH_AVAILABLE:
                self.hybrid_search = LightweightHybridSearch(
                    vector_store=self.vector_store,
                    embeddings=embeddings,
                    vector_weight=0.7,
                    keyword_weight=0.3
                )
                print("âœ… ä»ç¼“å­˜æˆåŠŸåŠ è½½çŸ¥è¯†åº“å’Œæ··åˆæ£€ç´¢å™¨")
            else:
                print("âœ… ä»ç¼“å­˜æˆåŠŸåŠ è½½çŸ¥è¯†åº“")
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°åˆå§‹åŒ–: {e}")
            return False
    
    def search_documents(
        self,
        query: str,
        k: int = 4,
        enable_rerank: bool = True,
        use_hybrid: bool = None
    ) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£ï¼Œæ”¯æŒæ··åˆæ£€ç´¢å’Œé‡æ’åº
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            enable_rerank: æ˜¯å¦å¯ç”¨é‡æ’åº
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®é»˜è®¤å€¼
        """
        if not self.vector_store:
            return []
        
        # ç¡®å®šæ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢
        if use_hybrid is None:
            use_hybrid = self.config.use_hybrid_search and self.hybrid_search is not None
        
        # æ£€æŸ¥æ··åˆæ£€ç´¢å¯ç”¨æ€§
        if use_hybrid and not self.hybrid_search:
            print("âš ï¸ æ··åˆæ£€ç´¢ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨çº¯å‘é‡æ£€ç´¢")
            use_hybrid = False
        
        try:
            print(f"ğŸ” å¼€å§‹æ–‡æ¡£æ£€ç´¢...")
            print(f"   æŸ¥è¯¢: {query}")
            print(f"   æ··åˆæ£€ç´¢: {'å¯ç”¨' if use_hybrid else 'ç¦ç”¨'}")
            print(f"   é‡æ’åº: {'å¯ç”¨' if enable_rerank and self.reranker else 'ç¦ç”¨'}")
            
            # ç¡®å®šåˆå§‹æ£€ç´¢æ•°é‡
            search_k = max(k * 2, 6) if enable_rerank and self.reranker else k
            
            if use_hybrid and self.hybrid_search:
                # ä½¿ç”¨æ··åˆæ£€ç´¢
                print(f"   ğŸ” ä½¿ç”¨æ··åˆæ£€ç´¢ï¼Œåˆå§‹æ£€ç´¢æ•°é‡: {search_k}")
                hybrid_results = self.hybrid_search.search(
                    query=query,
                    k=search_k,
                    fetch_k=search_k * 2
                )
                
                # è½¬æ¢ä¸ºDocumentå¯¹è±¡
                docs = [result.document for result in hybrid_results]
                
                # å­˜å‚¨æ··åˆæ£€ç´¢çš„åˆ†æ•°ä¿¡æ¯
                hybrid_scores = {
                    result.document.page_content: {
                        'vector_score': result.vector_score,
                        'keyword_score': result.keyword_score,
                        'combined_score': result.combined_score,
                        'match_type': result.match_type
                    }
                    for result in hybrid_results
                }
                
            else:
                # ä½¿ç”¨çº¯å‘é‡æ£€ç´¢
                print(f"   ğŸ“Š ä½¿ç”¨å‘é‡æ£€ç´¢ï¼Œæ£€ç´¢æ•°é‡: {search_k}")
                retriever = self.vector_store.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": search_k}
                )
                docs = retriever.invoke(query)
                hybrid_scores = {}
            
            print(f"   åˆå§‹æ£€ç´¢ç»“æœ: {len(docs)} ä¸ªæ–‡æ¡£")
            
            # åº”ç”¨é‡æ’åº
            if enable_rerank and self.reranker:
                print("âš–ï¸  å¼€å§‹é‡æ’åºå¤„ç†...")
                
                # è·å–é‡æ’åºä¿¡æ¯
                rerank_info = self.reranker.get_rerank_info(docs, query)
                
                # æ˜¾ç¤ºé‡æ’åºå‰çš„ä¿¡æ¯
                if use_hybrid and hybrid_scores:
                    print("   ğŸ“Š é‡æ’åºå‰ï¼ˆæ··åˆæ£€ç´¢åˆ†æ•°ï¼‰:")
                    for i, doc in enumerate(docs[:min(3, len(docs))]):
                        score_info = hybrid_scores.get(doc.page_content, {})
                        print(f"      {i+1}. {doc.metadata.get('source', 'æœªçŸ¥')} - "
                              f"ç»„åˆåˆ†æ•°: {score_info.get('combined_score', 0):.3f}, "
                              f"ç±»å‹: {score_info.get('match_type', 'unknown')}")
                
                # æ‰§è¡Œé‡æ’åº
                docs = self.reranker.rerank(docs, query)
                
                # æˆªå–å‰kä¸ªç»“æœ
                docs = docs[:k]
                print(f"   é‡æ’åºå: {len(docs)} ä¸ªæ–‡æ¡£")
            
            elif len(docs) > k:
                # å¦‚æœæ²¡æœ‰é‡æ’åºï¼Œç®€å•æˆªå–å‰kä¸ª
                docs = docs[:k]
            
            # æå–æ–‡æ¡£ä¿¡æ¯
            results = []
            print("ğŸ“‹ æœ€ç»ˆç»“æœ:")
            for i, doc in enumerate(docs):
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                
                # è·å–ç›¸å¯¹è·¯å¾„
                try:
                    if source != 'æœªçŸ¥æ¥æº':
                        source_path = Path(source)
                        if source_path.is_absolute():
                            try:
                                source = str(source_path.relative_to(self.config.knowledge_base_dir))
                            except ValueError:
                                source = source_path.name
                        else:
                            source = str(source_path)
                except Exception:
                    source = str(source)
                
                # å†…å®¹æ‘˜è¦
                content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                summary = content[:200] + "..." if len(content) > 200 else content
                
                # è·å–æ··åˆæ£€ç´¢åˆ†æ•°ä¿¡æ¯
                score_info = {}
                if hybrid_scores and doc.page_content in hybrid_scores:
                    score_info = hybrid_scores[doc.page_content]
                
                print(f"   {i+1}. {source} (é•¿åº¦: {len(content)} å­—ç¬¦)")
                if score_info:
                    print(f"      æ··åˆåˆ†æ•°: {score_info.get('combined_score', 0):.3f}, "
                          f"ç±»å‹: {score_info.get('match_type', 'unknown')}")
                
                results.append({
                    'source': source,
                    'content': content,
                    'summary': summary,
                    'length': len(content),
                    'metadata': metadata,
                    'hybrid_scores': score_info if score_info else None,
                    'search_method': 'hybrid' if use_hybrid else 'vector'
                })
            
            return results
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def get_search_stats(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "knowledge_base_ready": self.vector_store is not None,
            "hybrid_search_available": HYBRID_SEARCH_AVAILABLE,
            "hybrid_search_enabled": self.config.use_hybrid_search,
            "hybrid_search_ready": self.hybrid_search is not None,
            "reranker_ready": self.reranker is not None
        }
        
        if self.hybrid_search:
            stats.update(self.hybrid_search.get_stats())
        
        return stats
    
    def get_retriever(self, k: int = None):
        """è·å–æ£€ç´¢å™¨ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰
        
        Args:
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        """
        if self.vector_store:
            search_k = k if k is not None else self.config.k
            return self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": search_k}
            )
        return None
    
    def is_hybrid_available(self) -> bool:
        """æ£€æŸ¥æ··åˆæ£€ç´¢æ˜¯å¦å¯ç”¨"""
        return HYBRID_SEARCH_AVAILABLE and self.hybrid_search is not None
    
    def switch_search_mode(self, use_hybrid: bool):
        """åŠ¨æ€åˆ‡æ¢æ£€ç´¢æ¨¡å¼"""
        if use_hybrid and HYBRID_SEARCH_AVAILABLE and self.vector_store:
            if not self.hybrid_search:
                embeddings = EmbeddingFactory.create_embeddings(self.config)
                self.hybrid_search = LightweightHybridSearch(
                    vector_store=self.vector_store,
                    embeddings=embeddings,
                    vector_weight=0.7,
                    keyword_weight=0.3
                )
            print("âœ… å·²åˆ‡æ¢åˆ°æ··åˆæ£€ç´¢æ¨¡å¼")
        elif use_hybrid and not HYBRID_SEARCH_AVAILABLE:
            print("âš ï¸ æ··åˆæ£€ç´¢ä¸å¯ç”¨ï¼Œä¿æŒå½“å‰æ¨¡å¼")
        else:
            print("âœ… å·²åˆ‡æ¢åˆ°çº¯å‘é‡æ£€ç´¢æ¨¡å¼")
    
    # ä»¥ä¸‹æ–¹æ³•ä¿æŒç»Ÿä¸€å®ç°
    def _is_cache_valid(self) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        cache_marker = self.config.vector_db_dir / ".cache_valid"
        if not cache_marker.exists():
            return False
        
        cache_time = cache_marker.stat().st_mtime
        kb_time = max(
            (f.stat().st_mtime for f in Path(self.config.knowledge_base_dir).rglob("*") if f.is_file()),
            default=0
        )
        
        return cache_time > kb_time
    
    def _create_cache_marker(self):
        """åˆ›å»ºç¼“å­˜æ ‡è®°æ–‡ä»¶"""
        cache_marker = self.config.vector_db_dir / ".cache_valid"
        cache_marker.touch()
    
    def _load_documents(self) -> List:
        """åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£ï¼ˆç»Ÿä¸€å®ç°ï¼‰"""
        documents = []
        knowledge_base_path = Path(self.config.knowledge_base_dir)
        if not knowledge_base_path.exists():
            print(f"âš ï¸ çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {knowledge_base_path}")
            return []
        
        print(f"ğŸ“ æ­£åœ¨åŠ è½½çŸ¥è¯†åº“: {knowledge_base_path}")
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼å’Œå¯¹åº”çš„åŠ è½½å™¨
        file_configs = [
            {"pattern": "**/*.txt", "loader": TextLoader},
            {"pattern": "**/*.md", "loader": TextLoader},
            {"pattern": "**/*.capl", "loader": TextLoader},
            {"pattern": "**/*.py", "loader": TextLoader}
        ]
        
        # åŠ è½½éJSONæ–‡æ¡£
        for config in file_configs:
            pattern = config["pattern"]
            loader_cls = config["loader"]
            
            try:
                loader_kwargs = {'encoding': 'utf-8', 'autodetect_encoding': True}
                
                loader = DirectoryLoader(
                    str(knowledge_base_path),
                    glob=pattern,
                    loader_cls=loader_cls,
                    loader_kwargs=loader_kwargs
                )
                
                docs = loader.load()
                
                # ç¡®ä¿æ–‡æ¡£å†…å®¹æ ¼å¼æ­£ç¡®
                for doc in docs:
                    if hasattr(doc, 'page_content'):
                        content = doc.page_content
                        if not isinstance(content, str):
                            doc.page_content = str(content)
                        
                        # æ¸…ç†å†…å®¹
                        doc.page_content = doc.page_content.strip()
                        
                        # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                        if not doc.page_content:
                            continue
                
                # è¿‡æ»¤æ‰ç©ºæ–‡æ¡£
                valid_docs = [doc for doc in docs if doc.page_content and doc.page_content.strip()]
                documents.extend(valid_docs)
                
                if valid_docs:
                    print(f"ğŸ“ åŠ è½½ {pattern} æ ¼å¼: {len(valid_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
                    
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ {pattern} æ ¼å¼å¤±è´¥: {e}")
                continue
        
        # åŠ è½½JSONæ–‡æ¡£
        json_docs = self._load_json_documents()
        documents.extend(json_docs)
        
        print(f"ğŸ“Š æ€»å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def _load_json_documents(self) -> List:
        """ä½¿ç”¨JSONLoaderä¸“ä¸šåŠ è½½JSONæ–‡æ¡£"""
        documents = []
        
        # æ”¯æŒçš„JSONæ–‡ä»¶ç±»å‹
        json_patterns = ["**/*.json", "**/*.jsonl"]
        knowledge_base_path = Path(self.config.knowledge_base_dir)
        
        for pattern in json_patterns:
            try:
                json_files = list(knowledge_base_path.glob(pattern))
                for json_file in json_files:
                    try:
                        # ä½¿ç”¨JSONLoaderå¤„ç†JSONæ–‡æ¡£
                        loader = JSONLoader(
                            file_path=str(json_file),
                            jq_schema='.',
                            text_content=False,
                            content_key=None
                        )
                        docs = loader.load()
                        
                        # ä¸ºJSONæ–‡æ¡£æ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®
                        for doc in docs:
                            if hasattr(doc, 'metadata'):
                                doc.metadata.update({
                                    'document_type': 'json',
                                    'source': str(json_file.relative_to(knowledge_base_path)),
                                    'file_path': str(json_file),
                                    'format': 'structured_json'
                                })
                        
                        documents.extend(docs)
                        print(f"ğŸ“ JSONLoaderåŠ è½½: {json_file.name} ({len(docs)} ä¸ªæ–‡æ¡£)")
                        
                    except Exception as e:
                        # JSONLoaderå¤±è´¥æ—¶çš„å›é€€å¤„ç†
                        fallback_docs = self._fallback_json_processing(json_file)
                        documents.extend(fallback_docs)
                        print(f"âš ï¸ JSONLoaderå¤±è´¥ï¼Œä½¿ç”¨å›é€€æ¨¡å¼: {json_file.name} - {e}")
                        
            except Exception as e:
                print(f"âš ï¸ æŸ¥æ‰¾JSONæ–‡ä»¶å¤±è´¥: {e}")
                continue
                
        return documents
    
    def _fallback_json_processing(self, json_file: Path) -> List:
        """JSONLoaderå¤±è´¥æ—¶çš„å›é€€å¤„ç†"""
        documents = []
        try:
            import json
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # å°†JSONæ•°æ®è½¬æ¢ä¸ºç»“æ„åŒ–çš„æ–‡æœ¬è¡¨ç¤º
            content = json.dumps(data, ensure_ascii=False, indent=2)
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            doc = type('Document', (), {})()
            doc.page_content = content
            doc.metadata = {
                'document_type': 'json',
                'source': str(json_file.name),
                'file_path': str(json_file),
                'format': 'fallback_text'
            }
            documents.append(doc)
            
        except Exception as e:
            print(f"å›é€€å¤„ç†JSONå¤±è´¥: {json_file.name} - {e}")
        
        return documents