"""çŸ¥è¯†åº“ç®¡ç†å™¨ï¼Œå¤„ç†RAGç›¸å…³åŠŸèƒ½"""

import os
from pathlib import Path
from typing import Any, Dict, List

from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_community.document_loaders.json_loader import JSONLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

from ..config.config import CAPLGeneratorConfig
from ..factories.embedding_factory import EmbeddingFactory


class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨ï¼Œå¤„ç†RAGç›¸å…³åŠŸèƒ½"""
    
    def __init__(self, config: CAPLGeneratorConfig):
        self.config = config
        self.vector_store = None
        
    def initialize_knowledge_base(self) -> bool:
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        if not self.config.enable_rag:
            return False
            
        try:
            # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
            self.config.knowledge_base_dir.mkdir(exist_ok=True)
            self.config.vector_db_dir.mkdir(exist_ok=True)
            
            # åŠ è½½æ–‡æ¡£
            documents = self._load_documents()
            if not documents:
                print("è­¦å‘Š: çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
                return False
                
            # åˆ†å‰²æ–‡æ¡£
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len
            )
            splits = text_splitter.split_documents(documents)
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            embeddings = EmbeddingFactory.create_embeddings(self.config)
            
            # ä¿®å¤OllamaåµŒå…¥æ¨¡åž‹è¾“å…¥æ ¼å¼é—®é¢˜
            # ç¡®ä¿æ–‡æ¡£å†…å®¹éƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼
            for doc in splits:
                if hasattr(doc, 'page_content'):
                    # ç¡®ä¿å†…å®¹ä¸æ˜¯åˆ—è¡¨æˆ–å­—å…¸
                    if isinstance(doc.page_content, (list, dict)):
                        doc.page_content = str(doc.page_content)
                    # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²
                    doc.page_content = str(doc.page_content)
            
            # åˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆChroma 0.4+è‡ªåŠ¨æŒä¹…åŒ–ï¼‰
            self.vector_store = Chroma.from_documents(
                documents=splits,
                embedding=embeddings,
                persist_directory=str(self.config.vector_db_dir)
            )
            
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œ{len(splits)} ä¸ªæ–‡æœ¬å—")
            return True
            
        except Exception as e:
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _load_documents(self) -> List:
        """åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£"""
        if not self.config.knowledge_base_dir.exists():
            return []
            
        documents = []
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼å’Œå¯¹åº”çš„åŠ è½½å™¨
        file_configs = [
            {"pattern": "**/*.txt", "loader": TextLoader},
            {"pattern": "**/*.md", "loader": TextLoader},
            {"pattern": "**/*.capl", "loader": TextLoader},
            {"pattern": "**/*.py", "loader": TextLoader},
            {"pattern": "**/*.json", "loader": JSONLoader}
        ]
        
        for config in file_configs:
            pattern = config["pattern"]
            loader_cls = config["loader"]
            
            try:
                loader_kwargs = {'encoding': 'utf-8', 'autodetect_encoding': True}
                
                # ä¸ºJSONæ–‡ä»¶é…ç½®ç‰¹æ®Šçš„åŠ è½½å™¨å‚æ•°
                if loader_cls == JSONLoader:
                    # ä½¿ç”¨JSONLoaderå¤„ç†JSONæ–‡ä»¶ï¼Œæå–å†…å®¹ä½œä¸ºæ–‡æœ¬
                    loader = DirectoryLoader(
                        str(self.config.knowledge_base_dir),
                        glob=pattern,
                        loader_cls=TextLoader,  # å…ˆä½œä¸ºæ–‡æœ¬åŠ è½½ï¼Œç„¶åŽå¤„ç†
                        loader_kwargs=loader_kwargs
                    )
                else:
                    loader = DirectoryLoader(
                        str(self.config.knowledge_base_dir),
                        glob=pattern,
                        loader_cls=loader_cls,
                        loader_kwargs=loader_kwargs
                    )
                
                docs = loader.load()
                
                # ä¸ºJSONæ–‡ä»¶è¿›è¡Œç‰¹æ®Šå¤„ç†
                if pattern == "**/*.json":
                    docs = self._process_json_documents(docs)
                
                # ç¡®ä¿æ–‡æ¡£å†…å®¹æ ¼å¼æ­£ç¡®
                for doc in docs:
                    if hasattr(doc, 'page_content'):
                        # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯å¤æ‚å¯¹è±¡
                        content = doc.page_content
                        if isinstance(content, (list, dict, tuple)):
                            doc.page_content = self._format_json_content(content)
                        elif not isinstance(content, str):
                            doc.page_content = str(content)
                        
                        # æ¸…ç†å†…å®¹
                        doc.page_content = doc.page_content.strip()
                        
                        # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                        if not doc.page_content:
                            continue
                
                # è¿‡æ»¤æŽ‰ç©ºæ–‡æ¡£
                valid_docs = [doc for doc in docs if doc.page_content and doc.page_content.strip()]
                documents.extend(valid_docs)
                
                if valid_docs:
                    print(f"ðŸ“ åŠ è½½ {pattern} æ ¼å¼: {len(valid_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
                    
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ {pattern} æ ¼å¼å¤±è´¥: {e}")
                continue
        
        return documents
    
    def get_retriever(self, k: int = None):
        """èŽ·å–æ£€ç´¢å™¨
        
        Args:
            k: è¿”å›žçš„æ–‡æ¡£æ•°é‡ï¼Œå¦‚æžœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        """
        if self.vector_store:
            search_k = k if k is not None else self.config.k
            return self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": search_k}
            )
        return None
    
    def _process_json_documents(self, docs: List) -> List:
        """å¤„ç†JSONæ–‡æ¡£ï¼Œå°†å…¶è½¬æ¢ä¸ºå¯æœç´¢çš„æ–‡æœ¬æ ¼å¼"""
        processed_docs = []
        
        for doc in docs:
            if hasattr(doc, 'page_content'):
                try:
                    import json
                    content = doc.page_content
                    
                    # å°è¯•è§£æžJSONå†…å®¹
                    try:
                        json_data = json.loads(content)
                    except json.JSONDecodeError:
                        # å¦‚æžœä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œä¿æŒåŽŸæ ·
                        processed_docs.append(doc)
                        continue
                    
                    # å°†JSONæ•°æ®è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„æ–‡æœ¬
                    formatted_content = self._format_json_content(json_data)
                    
                    # æ›´æ–°æ–‡æ¡£å†…å®¹
                    doc.page_content = formatted_content
                    
                    # æ·»åŠ å…ƒæ•°æ®æ ‡è®°è¿™æ˜¯JSONæ–‡æ¡£
                    if hasattr(doc, 'metadata'):
                        doc.metadata['document_type'] = 'json'
                    
                    processed_docs.append(doc)
                    
                except Exception as e:
                    print(f"å¤„ç†JSONæ–‡æ¡£æ—¶å‡ºé”™: {e}")
                    processed_docs.append(doc)
        
        return processed_docs
    
    def _format_json_content(self, data: Any) -> str:
        """å°†JSONæ•°æ®æ ¼å¼åŒ–ä¸ºå¯æœç´¢çš„æ–‡æœ¬"""
        def _format_object(obj, indent=0):
            """é€’å½’æ ¼å¼åŒ–å¯¹è±¡"""
            result = []
            spaces = "  " * indent
            
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if isinstance(value, (dict, list)):
                        result.append(f"{spaces}{key}:")
                        result.append(_format_object(value, indent + 1))
                    else:
                        result.append(f"{spaces}{key}: {value}")
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    if isinstance(item, (dict, list)):
                        result.append(f"{spaces}[{i}]:")
                        result.append(_format_object(item, indent + 1))
                    else:
                        result.append(f"{spaces}[{i}]: {item}")
            else:
                result.append(f"{spaces}{obj}")
            
            return "\n".join(result)
        
        try:
            # å¦‚æžœæ˜¯åˆ—è¡¨æˆ–å­—å…¸ï¼Œä½¿ç”¨è‡ªå®šä¹‰æ ¼å¼åŒ–
            if isinstance(data, (dict, list)):
                return _format_object(data)
            else:
                return str(data)
        except Exception as e:
            print(f"æ ¼å¼åŒ–JSONå†…å®¹æ—¶å‡ºé”™: {e}")
            return str(data)
    
    def search_documents(self, query: str, k: int = 4) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£å¹¶è¿”å›žè¯¦ç»†ä¿¡æ¯"""
        if not self.vector_store:
            return []
        
        try:
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
            
            docs = retriever.invoke(query)
            
            # æå–æ–‡æ¡£ä¿¡æ¯
            results = []
            for doc in docs:
                # èŽ·å–æ–‡æ¡£å…ƒä¿¡æ¯
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                
                # èŽ·å–ç›¸å¯¹è·¯å¾„
                try:
                    if source != 'æœªçŸ¥æ¥æº':
                        source_path = Path(source)
                        if source_path.is_absolute():
                            # è½¬æ¢ä¸ºç›¸å¯¹äºŽçŸ¥è¯†åº“ç›®å½•çš„è·¯å¾„
                            try:
                                source = str(source_path.relative_to(self.config.knowledge_base_dir))
                            except ValueError:
                                source = source_path.name
                        else:
                            source = str(source_path)
                except Exception:
                    source = str(source)
                
                # å†…å®¹æ‘˜è¦
                content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                summary = content[:200] + "..." if len(content) > 200 else content
                
                results.append({
                    'source': source,
                    'content': content,
                    'summary': summary,
                    'length': len(content)
                })
            
            return results
            
        except Exception as e:
            print(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return []