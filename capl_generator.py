#!/usr/bin/env python3
import requests
import sys
import json
import os
from dotenv import load_dotenv
import ollama


def send_file_to_ollama(file_path):
    try:
        # åŠ è½½ .env æ–‡ä»¶
        load_dotenv()
        # è·å– API ç±»å‹å’Œ URL
        api_type = os.getenv('API_TYPE', 'ollama')  # å¯é€‰å€¼ï¼šollama æˆ– openai
        if api_type == 'ollama':
            default_url = 'http://localhost:11434/api/generate'
        else:  # openai å…¼å®¹çš„ API
            default_url = 'http://localhost:1234/v1/chat/completions'
        
        api_url = os.getenv('API_URL', default_url)

        with open(file_path, "r", encoding="utf-8") as file:
            file_content = file.read()
       
        # ä»é…ç½®æ–‡ä»¶è¯»å–æç¤ºè¯æ¨¡æ¿
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # å°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–æç¤ºè¯æ¨¡æ¿æ–‡ä»¶è·¯å¾„
        prompt_template_file = "prompt_template.txt"  # é»˜è®¤å€¼
        config_path = os.path.join(script_dir, "prompt_config.ini")
        
        if os.path.exists(config_path):
            try:
                import configparser
                config = configparser.ConfigParser()
                config.read(config_path, encoding='utf-8')
                if 'DEFAULT' in config and 'PROMPT_TEMPLATE_FILE' in config['DEFAULT']:
                    prompt_template_file = config['DEFAULT']['PROMPT_TEMPLATE_FILE']
            except Exception as e:
                print(f"è­¦å‘Š: è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯æ¨¡æ¿: {str(e)}")
        
        prompt_template_path = os.path.join(script_dir, prompt_template_file)
        
        try:
            with open(prompt_template_path, "r", encoding="utf-8") as prompt_file:
                prompt_template = prompt_file.read()
        except FileNotFoundError:
            return f"é”™è¯¯: æ‰¾ä¸åˆ°æç¤ºè¯æ¨¡æ¿æ–‡ä»¶ {prompt_template_path}"
        except Exception as e:
            return f"é”™è¯¯: è¯»å–æç¤ºè¯æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {str(e)}"

        if api_type == 'ollama':
            # ä½¿ç”¨å®˜æ–¹ ollama åº“
            # ä»API_URLä¸­æå–hoståœ°å€
            ollama_host = api_url.replace('/api/generate', '').replace('http://', '').replace('https://', '')
            # åˆ›å»ºollamaå®¢æˆ·ç«¯
            client = ollama.Client(host=f'http://{ollama_host}')
            
            stream = client.chat(
                model=os.getenv("OLLAMA_MODEL", "qwen3:30b-a3b"),
                messages=[
                    {"role": "system", "content": prompt_template},
                    {"role": "user", "content": file_content}
                ],
                stream=True,
                options={
                    "temperature": 0.2,
                    "top_p": 0.5,
                    "num_ctx": int(os.getenv("OLLAMA_CONTEXT_LENGTH", "8192")),  # é»˜è®¤8192ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®
                    "num_predict": int(os.getenv("OLLAMA_MAX_TOKENS", "4096")),  # é™åˆ¶æœ€å¤§è¾“å‡ºtokensï¼Œé˜²æ­¢æ— é™å¾ªç¯
                    "stop": ["```\n\n", "æµ‹è¯•ç”¨ä¾‹ç»“æŸ", "TestCaseç»“æŸ", "// End of test case"]  # åœæ­¢è¯ï¼Œé‡åˆ°è¿™äº›è¯å°±åœæ­¢ç”Ÿæˆ
                }
            )
        else:  # openai å…¼å®¹çš„ API
            payload = {
                "model": os.getenv("OPENAI_MODEL", "qwen/qwen3-1.7b"),
                "messages": [
                    {"role": "system", "content": prompt_template},
                    {"role": "user", "content": file_content}
                ],
                "stream": True,
                "temperature": 0.2,
                "top_p": 0.5
            }
            response = requests.post(api_url, json=payload, stream=True)
            response.raise_for_status()

        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # åˆ›å»º capl å­ç›®å½•åœ¨è„šæœ¬æ‰€åœ¨ç›®å½•ä¸‹
        capl_dir = os.path.join(script_dir, "capl")
        os.makedirs(capl_dir, exist_ok=True)
        
        # ç”Ÿæˆå¯¹åº”çš„ capl æ–‡ä»¶å
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        capl_file_path = os.path.join(capl_dir, f"{base_name}.md")
        
        with open(capl_file_path, "w", encoding="utf-8") as capl_file:
            if api_type == 'ollama':
                # å¤„ç† ollama åº“çš„æµå¼å“åº”
                for chunk in stream:
                    if 'message' in chunk and 'content' in chunk['message']:
                        response_text = chunk['message']['content']
                        print(response_text, end='', flush=True)
                        capl_file.write(response_text)
            else:
                # å¤„ç† openai å…¼å®¹ API çš„æµå¼å“åº”
                for line in response.iter_lines():
                    if line:
                        try:
                            data = line.decode('utf-8').lstrip('data: ')
                            if data:
                                json_data = json.loads(data)
                                if 'choices' in json_data and len(json_data['choices']) > 0:
                                    if 'delta' in json_data['choices'][0] and 'content' in json_data['choices'][0]['delta']:
                                        response_text = json_data['choices'][0]['delta']['content']
                                        print(response_text, end='', flush=True)
                                        capl_file.write(response_text)
                        except Exception as e:
                            continue
        return "\nå“åº”å®Œæˆ"
    except Exception as e:
        return f"å‘ç”Ÿé”™è¯¯: {str(e)}"

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("è¯·æä¾›æ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°ï¼Œä¾‹å¦‚ï¼špython capl_generator.py /path/to/your/file")
        sys.exit(1)
    file_path = sys.argv[1]
    result = send_file_to_ollama(file_path)
    if result.startswith("å‘ç”Ÿé”™è¯¯"):
        print(result)
    else:
        import subprocess
        import os
        script_dir = os.path.dirname(os.path.abspath(__file__))
        
        # è¿è¡Œ CAPL æå–å™¨
        subprocess.run(["python", os.path.join(script_dir, "capl_extractor.py")])
        
        # è¿è¡Œå¾ªç¯æ£€æµ‹å™¨æ¥æ£€æµ‹å’Œæ¸…ç†å¾ªç¯
        print("\nğŸ” æ­£åœ¨æ£€æµ‹å¾ªç¯æ¨¡å¼...")
        capl_dir = os.path.join(script_dir, "capl")
        if os.path.exists(capl_dir):
            for file in os.listdir(capl_dir):
                if file.endswith('.md'):
                    file_path = os.path.join(capl_dir, file)
                    subprocess.run(["python", os.path.join(script_dir, "loop_detector.py"), file_path, "--clean"])
        
        # è¿è¡Œ CAPL æ¸…ç†å™¨æ¥ä¿®å¤é‡å¤å®šä¹‰
        print("\nğŸ”§ æ­£åœ¨æ¸…ç†é‡å¤çš„å˜é‡å®šä¹‰...")
        subprocess.run(["python", os.path.join(script_dir, "capl_cleaner.py")])