#!/usr/bin/env python3
"""
åŸºäºLangChainçš„CAPLä»£ç ç”Ÿæˆå™¨
é‡æ„ç‰ˆæœ¬ï¼Œæ”¯æŒRAGå’Œæ›´çµæ´»çš„æ¶æ„
"""

import os
import sys
import json
import argparse
import time
from pathlib import Path
from typing import Optional, Dict, Any, List
from dotenv import load_dotenv

# LangChain imports - ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableSequence, RunnablePassthrough
from langchain_openai import ChatOpenAI
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_core.embeddings import Embeddings
from langchain_openai import OpenAIEmbeddings

# é…ç½®ç±»
class CAPLGeneratorConfig:
    """CAPLç”Ÿæˆå™¨é…ç½®ç±»"""
    
    def __init__(self):
        load_dotenv()
        
        # APIé…ç½®
        self.api_type = os.getenv('API_TYPE', 'ollama')  # ollama, openai
        self.api_url = os.getenv('API_URL', 'http://localhost:11434')
        self.model = os.getenv("OLLAMA_MODEL", "qwen3:30b-a3b")
        self.context_length = int(os.getenv("OLLAMA_CONTEXT_LENGTH", "8192"))
        self.max_tokens = int(os.getenv("OLLAMA_MAX_TOKENS", "4096"))
        self.temperature = float(os.getenv("TEMPERATURE", "0.2"))
        self.top_p = float(os.getenv("TOP_P", "0.5"))
        
        # è·¯å¾„é…ç½®
        self.project_root = Path(__file__).parent
        self.output_dir = self.project_root / "capl"
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–æç¤ºè¯æ¨¡æ¿è·¯å¾„
        self.prompt_template_file = self._get_prompt_template_path()
        self.example_code_file = self.project_root / "example_code.txt"
        
        # RAGé…ç½®
        self.enable_rag = os.getenv("ENABLE_RAG", "false").lower() == "true"
        self.knowledge_base_dir = self.project_root / "knowledge_base"
        self.vector_db_dir = self.project_root / "vector_db"
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")
    
    def _get_prompt_template_path(self) -> Path:
        """ä»é…ç½®æ–‡ä»¶è¯»å–æç¤ºè¯æ¨¡æ¿è·¯å¾„"""
        config_path = self.project_root / "prompt_config.ini"
        default_template = "prompt_template_simple.txt"
        
        if config_path.exists():
            try:
                import configparser
                config = configparser.ConfigParser()
                config.read(config_path, encoding='utf-8')
                if 'DEFAULT' in config and 'PROMPT_TEMPLATE_FILE' in config['DEFAULT']:
                    template_file = config['DEFAULT']['PROMPT_TEMPLATE_FILE']
                    return self.project_root / template_file
            except Exception as e:
                print(f"è­¦å‘Š: è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯æ¨¡æ¿: {e}")
        
        return self.project_root / default_template

class LLMFactory:
    """LLMå·¥å‚ç±»ï¼Œç»Ÿä¸€ç®¡ç†ä¸åŒæä¾›å•†çš„LLM"""
    
    @staticmethod
    def create_llm(config: CAPLGeneratorConfig) -> Any:
        """æ ¹æ®é…ç½®åˆ›å»ºLLMå®ä¾‹"""
        if config.api_type == "ollama":
            return OllamaLLM(
                base_url=config.api_url,
                model=config.model,
                temperature=config.temperature,
                top_p=config.top_p,
                num_ctx=config.context_length,
                num_predict=config.max_tokens
            )
        elif config.api_type == "openai":
            return ChatOpenAI(
                base_url=config.api_url,
                model=config.model,
                temperature=config.temperature,
                top_p=config.top_p,
                max_tokens=config.max_tokens
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„APIç±»å‹: {config.api_type}")

class EmbeddingFactory:
    """åµŒå…¥æ¨¡å‹å·¥å‚ç±»"""
    
    @staticmethod
    def create_embeddings(config: CAPLGeneratorConfig) -> Embeddings:
        """æ ¹æ®é…ç½®åˆ›å»ºåµŒå…¥æ¨¡å‹"""
        if config.api_type == "ollama":
            return OllamaEmbeddings(
                base_url=config.api_url,
                model=config.embedding_model
            )
        elif config.api_type == "openai":
            return OpenAIEmbeddings(
                base_url=config.api_url,
                model=config.embedding_model
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åµŒå…¥ç±»å‹: {config.api_type}")

class PromptTemplateManager:
    """æç¤ºæ¨¡æ¿ç®¡ç†å™¨"""
    
    def __init__(self, config: CAPLGeneratorConfig):
        self.config = config
        self.system_prompt = self._load_system_prompt()
        
    def _load_system_prompt(self) -> str:
        """åŠ è½½ç³»ç»Ÿæç¤ºæ¨¡æ¿"""
        try:
            if self.config.prompt_template_file.exists():
                with open(self.config.prompt_template_file, 'r', encoding='utf-8') as f:
                    prompt = f.read()
            else:
                prompt = self._get_default_prompt()
                
            # åŠ è½½ç¤ºä¾‹ä»£ç 
            if self.config.example_code_file.exists():
                with open(self.config.example_code_file, 'r', encoding='utf-8') as f:
                    example_code = f.read()
                    prompt = prompt.replace("ç¤ºä¾‹ä»£ç å·²ç§»è‡³å•ç‹¬çš„æ–‡ä»¶ example_code.txt ä¸­ï¼Œä»¥ä¿æŠ¤æ•æ„Ÿä»£ç å†…å®¹ã€‚", example_code)
            
            # è½¬ä¹‰éå˜é‡å ä½ç¬¦çš„èŠ±æ‹¬å·
            prompt = self._escape_brackets(prompt)
                    
            return prompt
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½æç¤ºæ¨¡æ¿å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
            return self._get_default_prompt()
    
    def _escape_brackets(self, text: str) -> str:
        """è½¬ä¹‰éå˜é‡å ä½ç¬¦çš„èŠ±æ‹¬å·
        
        LangChainä½¿ç”¨Jinja2æ¨¡æ¿å¼•æ“ï¼Œä¼šå°†å•èŠ±æ‹¬å·è¯†åˆ«ä¸ºå˜é‡å ä½ç¬¦ã€‚
        æ­¤æ–¹æ³•ä¼šè½¬ä¹‰æ‰€æœ‰éå˜é‡å ä½ç¬¦çš„èŠ±æ‹¬å·ï¼Œé¿å…è§£æé”™è¯¯ã€‚
        """
        import re
        
        # å®šä¹‰éœ€è¦ä¿ç•™çš„å˜é‡å ä½ç¬¦æ¨¡å¼
        variable_patterns = [
            r'\{requirement\}',
            r'\{context\}',
        ]
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶å ä½ç¬¦æ˜ å°„
        temp_placeholders = {}
        placeholder_counter = 0
        
        # é¦–å…ˆä¿æŠ¤å·²çŸ¥çš„å˜é‡å ä½ç¬¦
        protected_text = text
        for pattern in variable_patterns:
            matches = re.finditer(pattern, protected_text)
            for match in matches:
                placeholder = f"___VAR_PLACEHOLDER_{placeholder_counter}___"
                temp_placeholders[placeholder] = match.group()
                protected_text = protected_text.replace(match.group(), placeholder)
                placeholder_counter += 1
        
        # è½¬ä¹‰æ‰€æœ‰å‰©ä½™çš„å•èŠ±æ‹¬å·
        # å°† { æ›¿æ¢ä¸º {{ï¼Œ} æ›¿æ¢ä¸º }}
        protected_text = re.sub(r'(?<!\{)\{(?!\{)', '{{', protected_text)
        protected_text = re.sub(r'(?<!\})\}(?!\})', '}}', protected_text)
        
        # æ¢å¤è¢«ä¿æŠ¤çš„å˜é‡å ä½ç¬¦
        for placeholder, original in temp_placeholders.items():
            protected_text = protected_text.replace(placeholder, original)
        
        return protected_text
    
    def _get_default_prompt(self) -> str:
        """è·å–é»˜è®¤æç¤ºæ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„CAPLæµ‹è¯•ä»£ç ç”Ÿæˆä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„æµ‹è¯•éœ€æ±‚ï¼Œç”Ÿæˆé«˜è´¨é‡çš„CAPLæµ‹è¯•ä»£ç ã€‚

è¦æ±‚ï¼š
1. ä»£ç å¿…é¡»ç¬¦åˆCAPLè¯­æ³•è§„èŒƒ
2. åŒ…å«å®Œæ•´çš„æµ‹è¯•é€»è¾‘å’Œæ–­è¨€
3. æ·»åŠ è¯¦ç»†çš„æ³¨é‡Šè¯´æ˜
4. éµå¾ªæœ€ä½³å®è·µå’Œç¼–ç è§„èŒƒ

æµ‹è¯•éœ€æ±‚ï¼š
{requirement}

è¯·ç”Ÿæˆå¯¹åº”çš„CAPLæµ‹è¯•ä»£ç ï¼š"""

class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨ï¼Œå¤„ç†RAGç›¸å…³åŠŸèƒ½"""
    
    def __init__(self, config: CAPLGeneratorConfig):
        self.config = config
        self.vector_store = None
        
    def initialize_knowledge_base(self) -> bool:
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        if not self.config.enable_rag:
            return False
            
        try:
            # ç¡®ä¿çŸ¥è¯†åº“ç›®å½•å­˜åœ¨
            self.config.knowledge_base_dir.mkdir(exist_ok=True)
            self.config.vector_db_dir.mkdir(exist_ok=True)
            
            # åŠ è½½æ–‡æ¡£
            documents = self._load_documents()
            if not documents:
                print("è­¦å‘Š: çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
                return False
                
            # åˆ†å‰²æ–‡æ¡£
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len
            )
            splits = text_splitter.split_documents(documents)
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            embeddings = EmbeddingFactory.create_embeddings(self.config)
            
            # ä¿®å¤OllamaåµŒå…¥æ¨¡å‹è¾“å…¥æ ¼å¼é—®é¢˜
            # ç¡®ä¿æ–‡æ¡£å†…å®¹éƒ½æ˜¯å­—ç¬¦ä¸²æ ¼å¼
            for doc in splits:
                if hasattr(doc, 'page_content'):
                    # ç¡®ä¿å†…å®¹ä¸æ˜¯åˆ—è¡¨æˆ–å­—å…¸
                    if isinstance(doc.page_content, (list, dict)):
                        doc.page_content = str(doc.page_content)
                    # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²
                    doc.page_content = str(doc.page_content)
            
            # åˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆChroma 0.4+è‡ªåŠ¨æŒä¹…åŒ–ï¼‰
            self.vector_store = Chroma.from_documents(
                documents=splits,
                embedding=embeddings,
                persist_directory=str(self.config.vector_db_dir)
            )
            
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼Œå…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£ï¼Œ{len(splits)} ä¸ªæ–‡æœ¬å—")
            return True
            
        except Exception as e:
            print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _load_documents(self) -> List:
        """åŠ è½½çŸ¥è¯†åº“æ–‡æ¡£"""
        from langchain_community.document_loaders import DirectoryLoader, TextLoader
        
        if not self.config.knowledge_base_dir.exists():
            return []
            
        documents = []
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        file_patterns = ["**/*.txt", "**/*.md", "**/*.capl", "**/*.py"]
        
        for pattern in file_patterns:
            try:
                loader = DirectoryLoader(
                    str(self.config.knowledge_base_dir),
                    glob=pattern,
                    loader_cls=TextLoader,
                    loader_kwargs={'encoding': 'utf-8', 'autodetect_encoding': True}
                )
                docs = loader.load()
                
                # ç¡®ä¿æ–‡æ¡£å†…å®¹æ ¼å¼æ­£ç¡®
                for doc in docs:
                    if hasattr(doc, 'page_content'):
                        # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯å¤æ‚å¯¹è±¡
                        content = doc.page_content
                        if isinstance(content, (list, dict, tuple)):
                            doc.page_content = str(content)
                        elif not isinstance(content, str):
                            doc.page_content = str(content)
                        
                        # æ¸…ç†å†…å®¹
                        doc.page_content = doc.page_content.strip()
                        
                        # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
                        if not doc.page_content:
                            continue
                
                # è¿‡æ»¤æ‰ç©ºæ–‡æ¡£
                valid_docs = [doc for doc in docs if doc.page_content and doc.page_content.strip()]
                documents.extend(valid_docs)
                
                if valid_docs:
                    print(f"ğŸ“ åŠ è½½ {pattern} æ ¼å¼: {len(valid_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
                    
            except Exception as e:
                print(f"âš ï¸  åŠ è½½ {pattern} æ ¼å¼å¤±è´¥: {e}")
                continue
        
        return documents
    
    def get_retriever(self):
        """è·å–æ£€ç´¢å™¨"""
        if self.vector_store:
            return self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}
            )
        return None
    
    def search_documents(self, query: str, k: int = 4) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³æ–‡æ¡£å¹¶è¿”å›è¯¦ç»†ä¿¡æ¯"""
        if not self.vector_store:
            return []
        
        try:
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
            
            docs = retriever.invoke(query)
            
            # æå–æ–‡æ¡£ä¿¡æ¯
            results = []
            for doc in docs:
                # è·å–æ–‡æ¡£å…ƒä¿¡æ¯
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                
                # è·å–ç›¸å¯¹è·¯å¾„
                try:
                    if source != 'æœªçŸ¥æ¥æº':
                        source_path = Path(source)
                        if source_path.is_absolute():
                            # è½¬æ¢ä¸ºç›¸å¯¹äºçŸ¥è¯†åº“ç›®å½•çš„è·¯å¾„
                            try:
                                source = str(source_path.relative_to(self.config.knowledge_base_dir))
                            except ValueError:
                                source = source_path.name
                        else:
                            source = str(source_path)
                except Exception:
                    source = str(source)
                
                # å†…å®¹æ‘˜è¦
                content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                summary = content[:200] + "..." if len(content) > 200 else content
                
                results.append({
                    'source': source,
                    'content': content,
                    'summary': summary,
                    'length': len(content)
                })
            
            return results
            
        except Exception as e:
            print(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return []

class CAPLGenerator:
    """åŸºäºLangChainçš„CAPLä»£ç ç”Ÿæˆå™¨"""
    
    def __init__(self, config: Optional[CAPLGeneratorConfig] = None):
        self.config = config or CAPLGeneratorConfig()
        self.prompt_manager = PromptTemplateManager(self.config)
        self.kb_manager = KnowledgeBaseManager(self.config)
        self.llm = LLMFactory.create_llm(self.config)
        self.chain = None
        
    def initialize(self):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        # åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ˆå¦‚æœå¯ç”¨RAGï¼‰
        if self.config.enable_rag:
            self.kb_manager.initialize_knowledge_base()
        
        # æ„å»ºå¤„ç†é“¾
        self._build_chain()
    
    def _build_chain(self):
        """æ„å»ºLangChainå¤„ç†é“¾"""
        if self.config.enable_rag and self.kb_manager.get_retriever():
            # RAGæ¨¡å¼
            retriever = self.kb_manager.get_retriever()
            
            prompt = ChatPromptTemplate.from_messages([
                ("system", self.prompt_manager.system_prompt),
                ("human", """
                åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹ç”ŸæˆCAPLä»£ç ï¼š
                
                ç›¸å…³ä¸Šä¸‹æ–‡ï¼š
                {context}
                
                æµ‹è¯•éœ€æ±‚ï¼š
                {requirement}
                """)
            ])
            
            def format_docs(docs):
                return "\n\n".join(str(doc.page_content) for doc in docs)
            
            # ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®ï¼Œå¹¶æ˜¾ç¤ºæ£€ç´¢ä¿¡æ¯
            def create_chain_input(requirement_str):
                # ç¡®ä¿requirementæ˜¯å­—ç¬¦ä¸²
                if isinstance(requirement_str, dict):
                    requirement_str = str(requirement_str.get('requirement', str(requirement_str)))
                elif not isinstance(requirement_str, str):
                    requirement_str = str(requirement_str)
                
                # è·å–ç›¸å…³æ–‡æ¡£å¹¶æ˜¾ç¤ºæ£€ç´¢ä¿¡æ¯
                print(f"\nğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...")
                docs = retriever.invoke(requirement_str)
                
                if docs:
                    print(f"âœ… æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£")
                    
                    # æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„ä¿¡æ¯
                    for i, doc in enumerate(docs, 1):
                        # è·å–æ–‡æ¡£æ¥æº
                        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                        source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                        
                        # è·å–ç›¸å¯¹è·¯å¾„
                        try:
                            if source != 'æœªçŸ¥æ¥æº':
                                source_path = Path(source)
                                if source_path.is_absolute():
                                    try:
                                        source = str(source_path.relative_to(self.config.knowledge_base_dir))
                                    except ValueError:
                                        source = source_path.name
                                else:
                                    source = str(source_path)
                        except Exception:
                            source = str(source)
                        
                        # å†…å®¹æ‘˜è¦
                        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                        summary = content[:150] + "..." if len(content) > 150 else content
                        
                        print(f"   ğŸ“„ æ–‡æ¡£{i}: {source}")
                        print(f"      æ‘˜è¦: {summary}")
                        print(f"      é•¿åº¦: {len(content)} å­—ç¬¦")
                        print()
                else:
                    print("âš ï¸  æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå°†åŸºäºé€šç”¨çŸ¥è¯†ç”Ÿæˆ")
                
                context_str = format_docs(docs)
                
                return {
                    "context": context_str,
                    "requirement": requirement_str
                }
            
            # ä½¿ç”¨è‡ªå®šä¹‰çš„é“¾æ„å»º
            self.chain = (
                create_chain_input
                | prompt
                | self.llm
                | StrOutputParser()
            )
        else:
            # æ ‡å‡†æ¨¡å¼
            prompt = ChatPromptTemplate.from_messages([
                ("system", self.prompt_manager.system_prompt),
                ("human", "{requirement}")
            ])
            
            self.chain = prompt | self.llm | StrOutputParser()
    
    def generate_capl_code(self, requirement: str, context: Optional[str] = None) -> str:
        """ç”ŸæˆCAPLä»£ç """
        try:
            if not self.chain:
                self.initialize()
            
            # å‡†å¤‡è¾“å…¥
            if context:
                full_requirement = f"{context}\n\n{requirement}"
            else:
                full_requirement = requirement
            
            # ç”Ÿæˆä»£ç 
            response = self.chain.invoke({"requirement": full_requirement})
            
            return response
            
        except Exception as e:
            raise RuntimeError(f"CAPLä»£ç ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def generate_from_file(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è¯»å–éœ€æ±‚å¹¶ç”ŸæˆCAPLä»£ç """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                requirement = f.read()
            
            return self.generate_capl_code(requirement)
            
        except Exception as e:
            raise RuntimeError(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def get_document_info(self, query: str, k: int = 4) -> List[Dict[str, Any]]:
        """è·å–æ–‡æ¡£æ£€ç´¢ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•å’Œæ˜¾ç¤ºï¼‰"""
        if not self.config.enable_rag:
            print("âš ï¸  RAGåŠŸèƒ½æœªå¯ç”¨")
            return []
        
        if not self.kb_manager.vector_store:
            print("âš ï¸  çŸ¥è¯†åº“æœªåˆå§‹åŒ–")
            return []
        
        return self.kb_manager.search_documents(query, k)

class CAPLGeneratorService:
    """CAPLç”Ÿæˆå™¨æœåŠ¡ç±»ï¼Œæä¾›å®Œæ•´åŠŸèƒ½"""
    
    def __init__(self, config: Optional[CAPLGeneratorConfig] = None):
        self.config = config or CAPLGeneratorConfig()
        self.generator = CAPLGenerator(self.config)
        self.start_time = None
        
    def process_file(self, file_path: str, debug_prompt: bool = False, rebuild_rag: bool = False, **kwargs) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        self.start_time = time.time()
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.config.output_dir.mkdir(exist_ok=True)
            
            # å¤„ç†RAGé‡æ„é€‰é¡¹
            if rebuild_rag and self.config.enable_rag:
                print("ğŸ”„ æ£€æµ‹åˆ°RAGé‡æ„é€‰é¡¹ï¼Œæ­£åœ¨åˆ é™¤æ—§å‘é‡æ•°æ®åº“...")
                vector_db_path = self.config.vector_db_dir
                if vector_db_path.exists():
                    import shutil
                    shutil.rmtree(vector_db_path)
                    print(f"âœ… å·²åˆ é™¤æ—§å‘é‡æ•°æ®åº“: {vector_db_path}")
                else:
                    print("â„¹ï¸  å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤")
            
            # åˆå§‹åŒ–ç”Ÿæˆå™¨
            self.generator.initialize()
            
            # è¯»å–éœ€æ±‚æ–‡ä»¶
            requirement = self._read_file(file_path)
            
            # è°ƒè¯•æ¨¡å¼ï¼šæ‰“å°å®Œæ•´çš„prompt
            if debug_prompt:
                print("\nğŸ” è°ƒè¯•æ¨¡å¼ï¼šå®Œæ•´Promptå†…å®¹")
                print("=" * 60)
                print("ç³»ç»Ÿæç¤ºè¯:")
                print(self.generator.prompt_manager.system_prompt)
                print("\n" + "=" * 60)
                print("ç”¨æˆ·è¾“å…¥:")
                print(requirement)
                print("=" * 60)
                print("\n")
            
            # æ˜¾ç¤ºRAGçŠ¶æ€
            if self.config.enable_rag:
                print(f"\nğŸ“š RAGåŠŸèƒ½å·²å¯ç”¨")
                print(f"ğŸ“ çŸ¥è¯†åº“ç›®å½•: {self.config.knowledge_base_dir}")
                print(f"ğŸ“ å‘é‡æ•°æ®åº“ç›®å½•: {self.config.vector_db_dir}")
                
                # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
                vector_db_exists = self.config.vector_db_dir.exists() and \
                                 any(self.config.vector_db_dir.glob("*"))
                
                if vector_db_exists:
                    print("âœ… å‘é‡æ•°æ®åº“å·²å­˜åœ¨")
                    # åˆ—å‡ºæ•°æ®åº“æ–‡ä»¶
                    for item in self.config.vector_db_dir.rglob("*"):
                        if item.is_file():
                            size = item.stat().st_size
                            print(f"   ğŸ“„ {item.relative_to(self.config.vector_db_dir)} ({size} bytes)")
                else:
                    print("â„¹ï¸  å‘é‡æ•°æ®åº“ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„")
            else:
                print("â„¹ï¸  RAGåŠŸèƒ½æœªå¯ç”¨ï¼Œä½¿ç”¨é€šç”¨çŸ¥è¯†ç”Ÿæˆ")
            
            # ç”ŸæˆCAPLä»£ç 
            capl_code = self.generator.generate_capl_code(requirement)
            
            # ä¿å­˜ç»“æœ
            result = self._save_result(file_path, capl_code)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            stats = self._calculate_stats(capl_code)
            
            return {
                "status": "success",
                "file_path": str(result),
                "stats": stats,
                "error": None
            }
            
        except Exception as e:
            return {
                "status": "error",
                "file_path": None,
                "stats": {},
                "error": str(e)
            }
    
    def _read_file(self, file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _save_result(self, original_path: str, capl_code: str) -> Path:
        """ä¿å­˜ç”Ÿæˆçš„CAPLä»£ç 
        
        æ”¯æŒä¸¤ç§è¾“å‡ºæ ¼å¼ï¼š
        - .md æ ¼å¼ï¼šMarkdownæ ¼å¼çš„è¯¦ç»†è¯´æ˜æ–‡æ¡£
        - .can æ ¼å¼ï¼šçº¯CAPLä»£ç æ–‡ä»¶ï¼Œå¯ç›´æ¥ç”¨äºCANoe/CANalyzer
        """
        original_name = Path(original_path).stem
        
        # åŒæ—¶ç”Ÿæˆä¸¤ç§æ ¼å¼çš„æ–‡ä»¶
        md_file = self.config.output_dir / f"{original_name}.md"
        can_file = self.config.output_dir / f"{original_name}.can"
        
        # ä¿å­˜Markdownæ ¼å¼ï¼ˆåŒ…å«è¯¦ç»†è¯´æ˜ï¼‰
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(capl_code)
        
        # ä¿å­˜.canæ ¼å¼ï¼ˆçº¯ä»£ç ï¼‰
        # ä»ç”Ÿæˆçš„å†…å®¹ä¸­æå–ä»£ç å—
        import re
        code_blocks = re.findall(r'```(?:capl)?\n(.*?)\n```', capl_code, re.DOTALL)
        
        if code_blocks:
            # å¦‚æœæœ‰ä»£ç å—ï¼Œæå–ç¬¬ä¸€ä¸ªä»£ç å—ä½œä¸º.canæ–‡ä»¶å†…å®¹
            pure_code = code_blocks[0].strip()
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
            pure_code = capl_code
        
        with open(can_file, 'w', encoding='utf-8') as f:
            f.write(pure_code)
        
        return md_file  # è¿”å›ä¸»è¾“å‡ºæ–‡ä»¶è·¯å¾„
    
    def _calculate_stats(self, capl_code: str) -> Dict[str, Any]:
        """è®¡ç®—ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        generation_time = time.time() - self.start_time if self.start_time else 0
        code_length = len(capl_code)
        estimated_tokens = max(1, code_length // 4)
        token_rate = estimated_tokens / generation_time if generation_time > 0 else 0
        
        return {
            "generation_time": round(generation_time, 2),
            "code_length": code_length,
            "estimated_tokens": estimated_tokens,
            "token_rate": round(token_rate, 2)
        }
    
    def test_rag_search(self, query: str, k: int = 4) -> List[Dict[str, Any]]:
        """æµ‹è¯•RAGæœç´¢åŠŸèƒ½"""
        if not self.config.enable_rag:
            print("âš ï¸  RAGåŠŸèƒ½æœªå¯ç”¨")
            return []
        
        print(f"\nğŸ” æµ‹è¯•RAGæœç´¢: '{query}'")
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“
        self.generator.initialize()
        
        # æ‰§è¡Œæœç´¢
        results = self.generator.get_document_info(query, k)
        
        if results:
            print(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            for i, result in enumerate(results, 1):
                print(f"\nğŸ“„ æ–‡æ¡£{i}: {result['source']}")
                print(f"   æ‘˜è¦: {result['summary']}")
                print(f"   é•¿åº¦: {result['length']} å­—ç¬¦")
        else:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='åŸºäºLangChainçš„CAPLä»£ç ç”Ÿæˆå™¨')
    parser.add_argument('file_path', nargs='?', help='è¾“å…¥çš„æµ‹è¯•éœ€æ±‚æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--api-type', choices=['ollama', 'openai'], help='APIç±»å‹')
    parser.add_argument('--api-url', help='APIæœåŠ¡åœ°å€')
    parser.add_argument('--model', help='æ¨¡å‹åç§°')
    parser.add_argument('--enable-rag', action='store_true', help='å¯ç”¨RAGåŠŸèƒ½')
    parser.add_argument('--disable-rag', action='store_true', help='ç¦ç”¨RAGåŠŸèƒ½')
    parser.add_argument('--debug-prompt', action='store_true', help='è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºå®Œæ•´prompt')
    parser.add_argument('--rebuild-rag', action='store_true', help='é‡æ–°æ„å»ºRAGçŸ¥è¯†åº“')
    parser.add_argument('--test-rag', help='æµ‹è¯•RAGæœç´¢åŠŸèƒ½ï¼Œè¾“å…¥æŸ¥è¯¢å†…å®¹')
    parser.add_argument('--k', type=int, default=4, help='RAGæ£€ç´¢è¿”å›çš„æ–‡æ¡£æ•°é‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = CAPLGeneratorConfig()
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
    if args.api_type:
        config.api_type = args.api_type
    if args.api_url:
        config.api_url = args.api_url
    if args.model:
        config.model = args.model
    if args.enable_rag:
        config.enable_rag = True
    if args.disable_rag:
        config.enable_rag = False
    
    # åˆ›å»ºæœåŠ¡
    service = CAPLGeneratorService(config)
    
    # å¤„ç†ä¸åŒçš„æ“ä½œæ¨¡å¼
    if args.test_rag:
        # æµ‹è¯•RAGæœç´¢æ¨¡å¼
        service.test_rag_search(args.test_rag, args.k)
        return
    
    if not args.file_path:
        print("é”™è¯¯: è¯·æä¾›è¾“å…¥æ–‡ä»¶è·¯å¾„æˆ–ä½¿ç”¨ --test-rag æµ‹è¯•RAGåŠŸèƒ½")
        parser.print_help()
        return
    
    # æ­£å¸¸å¤„ç†æ–‡ä»¶
    print("=" * 60)
    print("CAPLä»£ç ç”Ÿæˆå™¨")
    print("=" * 60)
    
    result = service.process_file(
        args.file_path,
        debug_prompt=args.debug_prompt,
        rebuild_rag=args.rebuild_rag
    )
    
    if result["status"] == "success":
        print(f"âœ… ç”ŸæˆæˆåŠŸï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result['file_path']}")
        print(f"ğŸ“ CAPLä»£ç æ–‡ä»¶: {result['file_path'].replace('.md', '.can')}")
        print(f"â„¹ï¸  è¯´æ˜: .mdæ–‡ä»¶åŒ…å«è¯¦ç»†è¯´æ˜å’Œä»£ç ï¼Œ.canæ–‡ä»¶ä¸ºçº¯CAPLä»£ç å¯ç›´æ¥å¯¼å…¥CANoe/CANalyzer")
        print(f"â±ï¸  ç”Ÿæˆæ—¶é—´: {result['stats']['generation_time']}ç§’")
        print(f"ğŸ“Š ä»£ç é•¿åº¦: {result['stats']['code_length']}å­—ç¬¦")
        print(f"ğŸ”¢ ä¼°ç®—tokens: {result['stats']['estimated_tokens']}")
        print(f"âš¡ ç”Ÿæˆé€Ÿåº¦: {result['stats']['token_rate']} tokens/ç§’")
    else:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {result['error']}")
        sys.exit(1)

if __name__ == "__main__":
    main()